{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook trabajaremos con los datos de los marcadores e intentaremos clasificar las muestras de aceite de oliva con redes neuronales **Feed Forward**. Básicamente los pasos que seguiremos son los siguientes:\n",
    "- Lectura de datos ya normalizados\n",
    "- Preprocesamiento\n",
    "    - Encoding de los datos (modelo ternario/binario)\n",
    "- Entrenamiento de una red básica\n",
    "- Búsqueda de hiperparámetros\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de datos ya normalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>M6</th>\n",
       "      <th>M7</th>\n",
       "      <th>M8</th>\n",
       "      <th>M9</th>\n",
       "      <th>M10</th>\n",
       "      <th>...</th>\n",
       "      <th>M105</th>\n",
       "      <th>M106</th>\n",
       "      <th>M107</th>\n",
       "      <th>M108</th>\n",
       "      <th>M109</th>\n",
       "      <th>M110</th>\n",
       "      <th>M111</th>\n",
       "      <th>M112</th>\n",
       "      <th>M113</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.141858</td>\n",
       "      <td>0.117761</td>\n",
       "      <td>0.075981</td>\n",
       "      <td>0.220948</td>\n",
       "      <td>0.108434</td>\n",
       "      <td>0.038282</td>\n",
       "      <td>0.020016</td>\n",
       "      <td>0.042752</td>\n",
       "      <td>0.104742</td>\n",
       "      <td>0.072872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038282</td>\n",
       "      <td>0.047415</td>\n",
       "      <td>0.022542</td>\n",
       "      <td>0.009133</td>\n",
       "      <td>0.066848</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.078119</td>\n",
       "      <td>0.028566</td>\n",
       "      <td>0.045861</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.140789</td>\n",
       "      <td>0.110150</td>\n",
       "      <td>0.090602</td>\n",
       "      <td>0.178008</td>\n",
       "      <td>0.132519</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>0.022744</td>\n",
       "      <td>0.027256</td>\n",
       "      <td>0.067857</td>\n",
       "      <td>0.039662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027444</td>\n",
       "      <td>0.073872</td>\n",
       "      <td>0.014850</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>0.039286</td>\n",
       "      <td>0.052444</td>\n",
       "      <td>0.054887</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.049060</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.149605</td>\n",
       "      <td>0.135146</td>\n",
       "      <td>0.070947</td>\n",
       "      <td>0.198188</td>\n",
       "      <td>0.117794</td>\n",
       "      <td>0.033353</td>\n",
       "      <td>0.019665</td>\n",
       "      <td>0.038558</td>\n",
       "      <td>0.113939</td>\n",
       "      <td>0.075766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034702</td>\n",
       "      <td>0.027762</td>\n",
       "      <td>0.014459</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>0.048197</td>\n",
       "      <td>0.038558</td>\n",
       "      <td>0.078080</td>\n",
       "      <td>0.053788</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.048249</td>\n",
       "      <td>0.205985</td>\n",
       "      <td>0.180469</td>\n",
       "      <td>0.214103</td>\n",
       "      <td>0.197402</td>\n",
       "      <td>0.038506</td>\n",
       "      <td>0.022965</td>\n",
       "      <td>0.044305</td>\n",
       "      <td>0.083739</td>\n",
       "      <td>0.049408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050336</td>\n",
       "      <td>0.130596</td>\n",
       "      <td>0.025284</td>\n",
       "      <td>0.008815</td>\n",
       "      <td>0.025516</td>\n",
       "      <td>0.046161</td>\n",
       "      <td>0.088147</td>\n",
       "      <td>0.042218</td>\n",
       "      <td>0.061239</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010890</td>\n",
       "      <td>0.224685</td>\n",
       "      <td>0.090944</td>\n",
       "      <td>0.200994</td>\n",
       "      <td>0.109859</td>\n",
       "      <td>0.031525</td>\n",
       "      <td>0.016431</td>\n",
       "      <td>0.054834</td>\n",
       "      <td>0.064005</td>\n",
       "      <td>0.049484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025602</td>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.016431</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.043943</td>\n",
       "      <td>0.039740</td>\n",
       "      <td>0.073558</td>\n",
       "      <td>0.035155</td>\n",
       "      <td>0.071456</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         M1        M2        M3        M4        M5        M6        M7  \\\n",
       "0  0.141858  0.117761  0.075981  0.220948  0.108434  0.038282  0.020016   \n",
       "1  0.140789  0.110150  0.090602  0.178008  0.132519  0.032143  0.022744   \n",
       "2  0.149605  0.135146  0.070947  0.198188  0.117794  0.033353  0.019665   \n",
       "3  0.048249  0.205985  0.180469  0.214103  0.197402  0.038506  0.022965   \n",
       "4  0.010890  0.224685  0.090944  0.200994  0.109859  0.031525  0.016431   \n",
       "\n",
       "         M8        M9       M10  ...        M105      M106      M107  \\\n",
       "0  0.042752  0.104742  0.072872  ...    0.038282  0.047415  0.022542   \n",
       "1  0.027256  0.067857  0.039662  ...    0.027444  0.073872  0.014850   \n",
       "2  0.038558  0.113939  0.075766  ...    0.034702  0.027762  0.014459   \n",
       "3  0.044305  0.083739  0.049408  ...    0.050336  0.130596  0.025284   \n",
       "4  0.054834  0.064005  0.049484  ...    0.025602  0.113680  0.016431   \n",
       "\n",
       "       M108      M109      M110      M111      M112      M113  Class  \n",
       "0  0.009133  0.066848  0.052857  0.078119  0.028566  0.045861      E  \n",
       "1  0.007331  0.039286  0.052444  0.054887  0.028571  0.049060      E  \n",
       "2  0.009061  0.048197  0.038558  0.078080  0.053788  0.028533      E  \n",
       "3  0.008815  0.025516  0.046161  0.088147  0.042218  0.061239      E  \n",
       "4  0.006114  0.043943  0.039740  0.073558  0.035155  0.071456      E  \n",
       "\n",
       "[5 rows x 114 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizados = pd.read_excel('data/marcadores/normalizados.xlsx')\n",
    "normalizados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se comentó anteriormente, algunos algoritmos utilizados en minería de datos no trabajan bien con clases de tipo categóricas, es por ello por lo que hay que convertirlo a números. Haremos un encoding para distinguir entre cada uno de los modelos que entrenaremos (ternarios y binarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizados_L_nonL = normalizados.copy()\n",
    "normalizados_L_nonL.Class = normalizados_L_nonL.Class.map({ 'L':'1',\n",
    "                                     'V':'0',\n",
    "                                     'E':'0'                                     \n",
    "                                       })\n",
    "\n",
    "normalizados.Class = normalizados.Class.map({ 'L':'0',\n",
    "                                     'V':'1',\n",
    "                                     'E':'2'                                     \n",
    "                                       })\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>M6</th>\n",
       "      <th>M7</th>\n",
       "      <th>M8</th>\n",
       "      <th>M9</th>\n",
       "      <th>M10</th>\n",
       "      <th>...</th>\n",
       "      <th>M105</th>\n",
       "      <th>M106</th>\n",
       "      <th>M107</th>\n",
       "      <th>M108</th>\n",
       "      <th>M109</th>\n",
       "      <th>M110</th>\n",
       "      <th>M111</th>\n",
       "      <th>M112</th>\n",
       "      <th>M113</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.141858</td>\n",
       "      <td>0.117761</td>\n",
       "      <td>0.075981</td>\n",
       "      <td>0.220948</td>\n",
       "      <td>0.108434</td>\n",
       "      <td>0.038282</td>\n",
       "      <td>0.020016</td>\n",
       "      <td>0.042752</td>\n",
       "      <td>0.104742</td>\n",
       "      <td>0.072872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038282</td>\n",
       "      <td>0.047415</td>\n",
       "      <td>0.022542</td>\n",
       "      <td>0.009133</td>\n",
       "      <td>0.066848</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.078119</td>\n",
       "      <td>0.028566</td>\n",
       "      <td>0.045861</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.140789</td>\n",
       "      <td>0.110150</td>\n",
       "      <td>0.090602</td>\n",
       "      <td>0.178008</td>\n",
       "      <td>0.132519</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>0.022744</td>\n",
       "      <td>0.027256</td>\n",
       "      <td>0.067857</td>\n",
       "      <td>0.039662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027444</td>\n",
       "      <td>0.073872</td>\n",
       "      <td>0.014850</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>0.039286</td>\n",
       "      <td>0.052444</td>\n",
       "      <td>0.054887</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.049060</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.149605</td>\n",
       "      <td>0.135146</td>\n",
       "      <td>0.070947</td>\n",
       "      <td>0.198188</td>\n",
       "      <td>0.117794</td>\n",
       "      <td>0.033353</td>\n",
       "      <td>0.019665</td>\n",
       "      <td>0.038558</td>\n",
       "      <td>0.113939</td>\n",
       "      <td>0.075766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034702</td>\n",
       "      <td>0.027762</td>\n",
       "      <td>0.014459</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>0.048197</td>\n",
       "      <td>0.038558</td>\n",
       "      <td>0.078080</td>\n",
       "      <td>0.053788</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.048249</td>\n",
       "      <td>0.205985</td>\n",
       "      <td>0.180469</td>\n",
       "      <td>0.214103</td>\n",
       "      <td>0.197402</td>\n",
       "      <td>0.038506</td>\n",
       "      <td>0.022965</td>\n",
       "      <td>0.044305</td>\n",
       "      <td>0.083739</td>\n",
       "      <td>0.049408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050336</td>\n",
       "      <td>0.130596</td>\n",
       "      <td>0.025284</td>\n",
       "      <td>0.008815</td>\n",
       "      <td>0.025516</td>\n",
       "      <td>0.046161</td>\n",
       "      <td>0.088147</td>\n",
       "      <td>0.042218</td>\n",
       "      <td>0.061239</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010890</td>\n",
       "      <td>0.224685</td>\n",
       "      <td>0.090944</td>\n",
       "      <td>0.200994</td>\n",
       "      <td>0.109859</td>\n",
       "      <td>0.031525</td>\n",
       "      <td>0.016431</td>\n",
       "      <td>0.054834</td>\n",
       "      <td>0.064005</td>\n",
       "      <td>0.049484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025602</td>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.016431</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.043943</td>\n",
       "      <td>0.039740</td>\n",
       "      <td>0.073558</td>\n",
       "      <td>0.035155</td>\n",
       "      <td>0.071456</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         M1        M2        M3        M4        M5        M6        M7  \\\n",
       "0  0.141858  0.117761  0.075981  0.220948  0.108434  0.038282  0.020016   \n",
       "1  0.140789  0.110150  0.090602  0.178008  0.132519  0.032143  0.022744   \n",
       "2  0.149605  0.135146  0.070947  0.198188  0.117794  0.033353  0.019665   \n",
       "3  0.048249  0.205985  0.180469  0.214103  0.197402  0.038506  0.022965   \n",
       "4  0.010890  0.224685  0.090944  0.200994  0.109859  0.031525  0.016431   \n",
       "\n",
       "         M8        M9       M10  ...        M105      M106      M107  \\\n",
       "0  0.042752  0.104742  0.072872  ...    0.038282  0.047415  0.022542   \n",
       "1  0.027256  0.067857  0.039662  ...    0.027444  0.073872  0.014850   \n",
       "2  0.038558  0.113939  0.075766  ...    0.034702  0.027762  0.014459   \n",
       "3  0.044305  0.083739  0.049408  ...    0.050336  0.130596  0.025284   \n",
       "4  0.054834  0.064005  0.049484  ...    0.025602  0.113680  0.016431   \n",
       "\n",
       "       M108      M109      M110      M111      M112      M113  Class  \n",
       "0  0.009133  0.066848  0.052857  0.078119  0.028566  0.045861      2  \n",
       "1  0.007331  0.039286  0.052444  0.054887  0.028571  0.049060      2  \n",
       "2  0.009061  0.048197  0.038558  0.078080  0.053788  0.028533      2  \n",
       "3  0.008815  0.025516  0.046161  0.088147  0.042218  0.061239      2  \n",
       "4  0.006114  0.043943  0.039740  0.073558  0.035155  0.071456      2  \n",
       "\n",
       "[5 rows x 114 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de una red básica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos empezar con una red súper compleja, como esta:\n",
    "![title](img/Imagen8.png)\n",
    "Pero la experiencia nos dice que mejor es empezar con la estructura de red más básica, y a partir de ahí ir mejorando:\n",
    "![title](img/Imagen9.png)\n",
    "\n",
    "Lo primero que haremos es crear una red neuronal con tres capas:\n",
    "- 1 capa de entrada\n",
    "- 1 capa oculta\n",
    "- 1 capa de salida que realizará la predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que tendremos que hacer es dividir el conjunto de datos en train/test, en este caso tomaremos un 80% de los datos para train y el 20% restante para test. Sklearn nos proporciona un método que lo realiza automáticamente, se llama **train_test_split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo ternario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcadores_normalizados = normalizados.drop(columns=['Class'])\n",
    "y = normalizados.Class\n",
    "X_train, X_test, y_train, y_test = train_test_split(marcadores_normalizados, y, test_size=20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>M6</th>\n",
       "      <th>M7</th>\n",
       "      <th>M8</th>\n",
       "      <th>M9</th>\n",
       "      <th>M10</th>\n",
       "      <th>...</th>\n",
       "      <th>M105</th>\n",
       "      <th>M106</th>\n",
       "      <th>M107</th>\n",
       "      <th>M108</th>\n",
       "      <th>M109</th>\n",
       "      <th>M110</th>\n",
       "      <th>M111</th>\n",
       "      <th>M112</th>\n",
       "      <th>M113</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.141858</td>\n",
       "      <td>0.117761</td>\n",
       "      <td>0.075981</td>\n",
       "      <td>0.220948</td>\n",
       "      <td>0.108434</td>\n",
       "      <td>0.038282</td>\n",
       "      <td>0.020016</td>\n",
       "      <td>0.042752</td>\n",
       "      <td>0.104742</td>\n",
       "      <td>0.072872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038282</td>\n",
       "      <td>0.047415</td>\n",
       "      <td>0.022542</td>\n",
       "      <td>0.009133</td>\n",
       "      <td>0.066848</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.078119</td>\n",
       "      <td>0.028566</td>\n",
       "      <td>0.045861</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.140789</td>\n",
       "      <td>0.110150</td>\n",
       "      <td>0.090602</td>\n",
       "      <td>0.178008</td>\n",
       "      <td>0.132519</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>0.022744</td>\n",
       "      <td>0.027256</td>\n",
       "      <td>0.067857</td>\n",
       "      <td>0.039662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027444</td>\n",
       "      <td>0.073872</td>\n",
       "      <td>0.014850</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>0.039286</td>\n",
       "      <td>0.052444</td>\n",
       "      <td>0.054887</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.049060</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.149605</td>\n",
       "      <td>0.135146</td>\n",
       "      <td>0.070947</td>\n",
       "      <td>0.198188</td>\n",
       "      <td>0.117794</td>\n",
       "      <td>0.033353</td>\n",
       "      <td>0.019665</td>\n",
       "      <td>0.038558</td>\n",
       "      <td>0.113939</td>\n",
       "      <td>0.075766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034702</td>\n",
       "      <td>0.027762</td>\n",
       "      <td>0.014459</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>0.048197</td>\n",
       "      <td>0.038558</td>\n",
       "      <td>0.078080</td>\n",
       "      <td>0.053788</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.048249</td>\n",
       "      <td>0.205985</td>\n",
       "      <td>0.180469</td>\n",
       "      <td>0.214103</td>\n",
       "      <td>0.197402</td>\n",
       "      <td>0.038506</td>\n",
       "      <td>0.022965</td>\n",
       "      <td>0.044305</td>\n",
       "      <td>0.083739</td>\n",
       "      <td>0.049408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050336</td>\n",
       "      <td>0.130596</td>\n",
       "      <td>0.025284</td>\n",
       "      <td>0.008815</td>\n",
       "      <td>0.025516</td>\n",
       "      <td>0.046161</td>\n",
       "      <td>0.088147</td>\n",
       "      <td>0.042218</td>\n",
       "      <td>0.061239</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010890</td>\n",
       "      <td>0.224685</td>\n",
       "      <td>0.090944</td>\n",
       "      <td>0.200994</td>\n",
       "      <td>0.109859</td>\n",
       "      <td>0.031525</td>\n",
       "      <td>0.016431</td>\n",
       "      <td>0.054834</td>\n",
       "      <td>0.064005</td>\n",
       "      <td>0.049484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025602</td>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.016431</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.043943</td>\n",
       "      <td>0.039740</td>\n",
       "      <td>0.073558</td>\n",
       "      <td>0.035155</td>\n",
       "      <td>0.071456</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         M1        M2        M3        M4        M5        M6        M7  \\\n",
       "0  0.141858  0.117761  0.075981  0.220948  0.108434  0.038282  0.020016   \n",
       "1  0.140789  0.110150  0.090602  0.178008  0.132519  0.032143  0.022744   \n",
       "2  0.149605  0.135146  0.070947  0.198188  0.117794  0.033353  0.019665   \n",
       "3  0.048249  0.205985  0.180469  0.214103  0.197402  0.038506  0.022965   \n",
       "4  0.010890  0.224685  0.090944  0.200994  0.109859  0.031525  0.016431   \n",
       "\n",
       "         M8        M9       M10  ...        M105      M106      M107  \\\n",
       "0  0.042752  0.104742  0.072872  ...    0.038282  0.047415  0.022542   \n",
       "1  0.027256  0.067857  0.039662  ...    0.027444  0.073872  0.014850   \n",
       "2  0.038558  0.113939  0.075766  ...    0.034702  0.027762  0.014459   \n",
       "3  0.044305  0.083739  0.049408  ...    0.050336  0.130596  0.025284   \n",
       "4  0.054834  0.064005  0.049484  ...    0.025602  0.113680  0.016431   \n",
       "\n",
       "       M108      M109      M110      M111      M112      M113  Class  \n",
       "0  0.009133  0.066848  0.052857  0.078119  0.028566  0.045861      2  \n",
       "1  0.007331  0.039286  0.052444  0.054887  0.028571  0.049060      2  \n",
       "2  0.009061  0.048197  0.038558  0.078080  0.053788  0.028533      2  \n",
       "3  0.008815  0.025516  0.046161  0.088147  0.042218  0.061239      2  \n",
       "4  0.006114  0.043943  0.039740  0.073558  0.035155  0.071456      2  \n",
       "\n",
       "[5 rows x 114 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 513 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      "513/513 [==============================] - 1s 1ms/step - loss: 1.0919 - acc: 0.4074 - val_loss: 1.1038 - val_acc: 0.4000\n",
      "Epoch 2/10\n",
      "513/513 [==============================] - 0s 62us/step - loss: 1.0138 - acc: 0.5205 - val_loss: 1.0938 - val_acc: 0.4000\n",
      "Epoch 3/10\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.9900 - acc: 0.5205 - val_loss: 1.0572 - val_acc: 0.4000\n",
      "Epoch 4/10\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.9695 - acc: 0.5205 - val_loss: 1.0590 - val_acc: 0.4000\n",
      "Epoch 5/10\n",
      "513/513 [==============================] - 0s 63us/step - loss: 0.9508 - acc: 0.5244 - val_loss: 1.0430 - val_acc: 0.4000\n",
      "Epoch 6/10\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.9293 - acc: 0.5341 - val_loss: 1.0300 - val_acc: 0.4000\n",
      "Epoch 7/10\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.9081 - acc: 0.5906 - val_loss: 0.9981 - val_acc: 0.5500\n",
      "Epoch 8/10\n",
      "513/513 [==============================] - 0s 63us/step - loss: 0.8965 - acc: 0.6296 - val_loss: 0.9894 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.8779 - acc: 0.6179 - val_loss: 0.9802 - val_acc: 0.4500\n",
      "Epoch 10/10\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.8555 - acc: 0.6179 - val_loss: 0.9856 - val_acc: 0.5500\n",
      "20/20 [==============================] - 0s 200us/step\n",
      "\n",
      "acc: 55.00%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() ## Inicializamos el modelo\n",
    "model.add(Dense(50, input_dim=113, activation=\"relu\")) # Capa oculta\n",
    "model.add(Dense(3, activation = 'softmax')) # Capa de salida con 3 neuronas (tenemos 3 clases en el modelo ternario)\n",
    "    \n",
    "# compilar el modelo\n",
    "model.compile(optimizer=\"Adam\",loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train, epochs=10,verbose=1, validation_data = (X_test, y_test))\n",
    "\n",
    "## Evaluamos el modelo\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos insertado unos parámetros al azar. El primer parámetro que vamos a tener en cuenta es el número de **epochs**, para entender mejor qué significa echemos un vistazo a la siguiente imagen\n",
    "![title](img/Imagen10.png)\n",
    "Lo más lógico es pensar que a más epochs, el accuracy es mayor, y en cierta manera es así, pero ¿cuál es el número óptimo de epochs?. El proceso a seguir es simple, ejecutar el algoritmo para diferentes números de epochs y ver cómo se comporta la red neuronal, representando en una gráfica el accuracy obtenido en base al número de estos. <br>\n",
    "Otro aspecto a tener en cuenta es que llega un momento en el que el algoritmo se sobreentrena, es aquí cuando hay que parar. Este concepto se puede ver mejor en la siguiente imagen:\n",
    "![title](img/Imagen11.png)\n",
    "\n",
    "Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 513 samples, validate on 20 samples\n",
      "Epoch 1/500\n",
      "513/513 [==============================] - 1s 1ms/step - loss: 1.0316 - acc: 0.5263 - val_loss: 1.1356 - val_acc: 0.4000\n",
      "Epoch 2/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.9928 - acc: 0.5205 - val_loss: 1.0985 - val_acc: 0.4000\n",
      "Epoch 3/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.9644 - acc: 0.5224 - val_loss: 1.0805 - val_acc: 0.4000\n",
      "Epoch 4/500\n",
      "513/513 [==============================] - 0s 67us/step - loss: 0.9451 - acc: 0.5322 - val_loss: 1.0610 - val_acc: 0.4500\n",
      "Epoch 5/500\n",
      "513/513 [==============================] - 0s 81us/step - loss: 0.9268 - acc: 0.5750 - val_loss: 1.0442 - val_acc: 0.4500\n",
      "Epoch 6/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.9095 - acc: 0.6043 - val_loss: 1.0289 - val_acc: 0.4500\n",
      "Epoch 7/500\n",
      "513/513 [==============================] - 0s 79us/step - loss: 0.8937 - acc: 0.5867 - val_loss: 1.0397 - val_acc: 0.4500\n",
      "Epoch 8/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.8758 - acc: 0.5848 - val_loss: 1.0347 - val_acc: 0.4500\n",
      "Epoch 9/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.8618 - acc: 0.5965 - val_loss: 1.0062 - val_acc: 0.4500\n",
      "Epoch 10/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.8573 - acc: 0.6511 - val_loss: 0.9924 - val_acc: 0.4500\n",
      "Epoch 11/500\n",
      "513/513 [==============================] - 0s 80us/step - loss: 0.8352 - acc: 0.6004 - val_loss: 1.0098 - val_acc: 0.5500\n",
      "Epoch 12/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.8246 - acc: 0.6608 - val_loss: 0.9757 - val_acc: 0.5500\n",
      "Epoch 13/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.8075 - acc: 0.6452 - val_loss: 0.9917 - val_acc: 0.5500\n",
      "Epoch 14/500\n",
      "513/513 [==============================] - 0s 80us/step - loss: 0.7978 - acc: 0.6394 - val_loss: 0.9905 - val_acc: 0.5500\n",
      "Epoch 15/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.7824 - acc: 0.6550 - val_loss: 0.9559 - val_acc: 0.5500\n",
      "Epoch 16/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.7701 - acc: 0.6764 - val_loss: 0.9653 - val_acc: 0.5500\n",
      "Epoch 17/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.7614 - acc: 0.6628 - val_loss: 0.9542 - val_acc: 0.5500\n",
      "Epoch 18/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.7503 - acc: 0.6725 - val_loss: 0.9633 - val_acc: 0.5500\n",
      "Epoch 19/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.7397 - acc: 0.6920 - val_loss: 0.9369 - val_acc: 0.6000\n",
      "Epoch 20/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.7334 - acc: 0.6764 - val_loss: 0.9355 - val_acc: 0.6000\n",
      "Epoch 21/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.7273 - acc: 0.7310 - val_loss: 0.9226 - val_acc: 0.6000\n",
      "Epoch 22/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.7126 - acc: 0.6803 - val_loss: 0.9428 - val_acc: 0.6000\n",
      "Epoch 23/500\n",
      "513/513 [==============================] - 0s 77us/step - loss: 0.7004 - acc: 0.7193 - val_loss: 0.9237 - val_acc: 0.6000\n",
      "Epoch 24/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.6943 - acc: 0.7154 - val_loss: 0.9191 - val_acc: 0.6000\n",
      "Epoch 25/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.6902 - acc: 0.6920 - val_loss: 0.8997 - val_acc: 0.6000\n",
      "Epoch 26/500\n",
      "513/513 [==============================] - 0s 84us/step - loss: 0.6760 - acc: 0.7407 - val_loss: 0.9022 - val_acc: 0.6000\n",
      "Epoch 27/500\n",
      "513/513 [==============================] - 0s 82us/step - loss: 0.6750 - acc: 0.7057 - val_loss: 0.9160 - val_acc: 0.6000\n",
      "Epoch 28/500\n",
      "513/513 [==============================] - 0s 116us/step - loss: 0.6608 - acc: 0.7368 - val_loss: 0.8974 - val_acc: 0.6000\n",
      "Epoch 29/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.6583 - acc: 0.7524 - val_loss: 0.8841 - val_acc: 0.6500\n",
      "Epoch 30/500\n",
      "513/513 [==============================] - 0s 80us/step - loss: 0.6510 - acc: 0.7563 - val_loss: 0.8532 - val_acc: 0.6500\n",
      "Epoch 31/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.6604 - acc: 0.7466 - val_loss: 0.8785 - val_acc: 0.6000\n",
      "Epoch 32/500\n",
      "513/513 [==============================] - 0s 87us/step - loss: 0.6406 - acc: 0.7466 - val_loss: 0.8842 - val_acc: 0.6500\n",
      "Epoch 33/500\n",
      "513/513 [==============================] - 0s 78us/step - loss: 0.6368 - acc: 0.7544 - val_loss: 0.8851 - val_acc: 0.6000\n",
      "Epoch 34/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.6271 - acc: 0.7524 - val_loss: 0.8475 - val_acc: 0.6500\n",
      "Epoch 35/500\n",
      "513/513 [==============================] - 0s 67us/step - loss: 0.6177 - acc: 0.7739 - val_loss: 0.8854 - val_acc: 0.6000\n",
      "Epoch 36/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.6190 - acc: 0.7524 - val_loss: 0.8252 - val_acc: 0.6500\n",
      "Epoch 37/500\n",
      "513/513 [==============================] - 0s 58us/step - loss: 0.6326 - acc: 0.7524 - val_loss: 0.8418 - val_acc: 0.6500\n",
      "Epoch 38/500\n",
      "513/513 [==============================] - 0s 78us/step - loss: 0.6126 - acc: 0.7583 - val_loss: 0.8784 - val_acc: 0.6500\n",
      "Epoch 39/500\n",
      "513/513 [==============================] - 0s 77us/step - loss: 0.6041 - acc: 0.7719 - val_loss: 0.8179 - val_acc: 0.6500\n",
      "Epoch 40/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.5944 - acc: 0.7739 - val_loss: 0.8635 - val_acc: 0.6500\n",
      "Epoch 41/500\n",
      "513/513 [==============================] - 0s 79us/step - loss: 0.5922 - acc: 0.7700 - val_loss: 0.8261 - val_acc: 0.6500\n",
      "Epoch 42/500\n",
      "513/513 [==============================] - 0s 77us/step - loss: 0.5857 - acc: 0.7739 - val_loss: 0.8297 - val_acc: 0.6500\n",
      "Epoch 43/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.5818 - acc: 0.7739 - val_loss: 0.8172 - val_acc: 0.6500\n",
      "Epoch 44/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.5790 - acc: 0.7778 - val_loss: 0.8379 - val_acc: 0.6500\n",
      "Epoch 45/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.5893 - acc: 0.7583 - val_loss: 0.8187 - val_acc: 0.6500\n",
      "Epoch 46/500\n",
      "513/513 [==============================] - 0s 77us/step - loss: 0.5773 - acc: 0.7758 - val_loss: 0.8342 - val_acc: 0.6500\n",
      "Epoch 47/500\n",
      "513/513 [==============================] - 0s 77us/step - loss: 0.5678 - acc: 0.7700 - val_loss: 0.8221 - val_acc: 0.6500\n",
      "Epoch 48/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.5638 - acc: 0.7914 - val_loss: 0.8012 - val_acc: 0.6500\n",
      "Epoch 49/500\n",
      "513/513 [==============================] - 0s 77us/step - loss: 0.5669 - acc: 0.7836 - val_loss: 0.8132 - val_acc: 0.6500\n",
      "Epoch 50/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.5657 - acc: 0.7817 - val_loss: 0.7821 - val_acc: 0.6500\n",
      "Epoch 51/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.5771 - acc: 0.7680 - val_loss: 0.7621 - val_acc: 0.6500\n",
      "Epoch 52/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.5784 - acc: 0.7524 - val_loss: 0.8672 - val_acc: 0.6500\n",
      "Epoch 53/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.5578 - acc: 0.7583 - val_loss: 0.7834 - val_acc: 0.6500\n",
      "Epoch 54/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.5473 - acc: 0.7778 - val_loss: 0.7754 - val_acc: 0.6500\n",
      "Epoch 55/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.5635 - acc: 0.7914 - val_loss: 0.7677 - val_acc: 0.6500\n",
      "Epoch 56/500\n",
      "513/513 [==============================] - 0s 63us/step - loss: 0.5541 - acc: 0.7836 - val_loss: 0.8468 - val_acc: 0.6500\n",
      "Epoch 57/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.5414 - acc: 0.7895 - val_loss: 0.7292 - val_acc: 0.7000\n",
      "Epoch 58/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.5639 - acc: 0.7661 - val_loss: 0.8023 - val_acc: 0.6500\n",
      "Epoch 59/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.5473 - acc: 0.7719 - val_loss: 0.7763 - val_acc: 0.6500\n",
      "Epoch 60/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.5331 - acc: 0.7895 - val_loss: 0.7880 - val_acc: 0.6500\n",
      "Epoch 61/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513/513 [==============================] - 0s 77us/step - loss: 0.5687 - acc: 0.7427 - val_loss: 0.7530 - val_acc: 0.6500\n",
      "Epoch 62/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.5651 - acc: 0.7934 - val_loss: 0.7552 - val_acc: 0.6500\n",
      "Epoch 63/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.5366 - acc: 0.7934 - val_loss: 0.7931 - val_acc: 0.6500\n",
      "Epoch 64/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.5348 - acc: 0.7895 - val_loss: 0.7645 - val_acc: 0.6500\n",
      "Epoch 65/500\n",
      "513/513 [==============================] - 0s 83us/step - loss: 0.5444 - acc: 0.7758 - val_loss: 0.7830 - val_acc: 0.6500\n",
      "Epoch 66/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.5310 - acc: 0.7992 - val_loss: 0.7407 - val_acc: 0.6500\n",
      "Epoch 67/500\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.5319 - acc: 0.7875 - val_loss: 0.7722 - val_acc: 0.6500\n",
      "Epoch 68/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.5352 - acc: 0.7973 - val_loss: 0.7317 - val_acc: 0.7000\n",
      "Epoch 69/500\n",
      "513/513 [==============================] - 0s 67us/step - loss: 0.5266 - acc: 0.7856 - val_loss: 0.8138 - val_acc: 0.6500\n",
      "Epoch 70/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.5242 - acc: 0.7797 - val_loss: 0.7504 - val_acc: 0.6500\n",
      "Epoch 71/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.5148 - acc: 0.7953 - val_loss: 0.7771 - val_acc: 0.6500\n",
      "Epoch 72/500\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.5157 - acc: 0.7992 - val_loss: 0.7573 - val_acc: 0.6500\n",
      "Epoch 73/500\n",
      "513/513 [==============================] - 0s 67us/step - loss: 0.5113 - acc: 0.7953 - val_loss: 0.7924 - val_acc: 0.6500\n",
      "Epoch 74/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.5164 - acc: 0.7973 - val_loss: 0.7820 - val_acc: 0.6500\n",
      "Epoch 75/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.5143 - acc: 0.7817 - val_loss: 0.7453 - val_acc: 0.7000\n",
      "Epoch 76/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.5069 - acc: 0.7992 - val_loss: 0.7671 - val_acc: 0.6500\n",
      "Epoch 77/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.5086 - acc: 0.7953 - val_loss: 0.7322 - val_acc: 0.7500\n",
      "Epoch 78/500\n",
      "513/513 [==============================] - 0s 89us/step - loss: 0.5035 - acc: 0.7992 - val_loss: 0.7560 - val_acc: 0.6500\n",
      "Epoch 79/500\n",
      "513/513 [==============================] - 0s 87us/step - loss: 0.5120 - acc: 0.7973 - val_loss: 0.7616 - val_acc: 0.6500\n",
      "Epoch 80/500\n",
      "513/513 [==============================] - 0s 78us/step - loss: 0.5098 - acc: 0.7914 - val_loss: 0.7638 - val_acc: 0.6500\n",
      "Epoch 81/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.5031 - acc: 0.7973 - val_loss: 0.7544 - val_acc: 0.6500\n",
      "Epoch 82/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.5165 - acc: 0.7700 - val_loss: 0.7424 - val_acc: 0.7000\n",
      "Epoch 83/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.4979 - acc: 0.7992 - val_loss: 0.7348 - val_acc: 0.7000\n",
      "Epoch 84/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.5047 - acc: 0.8012 - val_loss: 0.7297 - val_acc: 0.7000\n",
      "Epoch 85/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.5011 - acc: 0.7973 - val_loss: 0.7982 - val_acc: 0.7000\n",
      "Epoch 86/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.4994 - acc: 0.7973 - val_loss: 0.7546 - val_acc: 0.6500\n",
      "Epoch 87/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.4926 - acc: 0.7953 - val_loss: 0.7442 - val_acc: 0.7000\n",
      "Epoch 88/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4971 - acc: 0.7875 - val_loss: 0.7402 - val_acc: 0.7000\n",
      "Epoch 89/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4905 - acc: 0.8031 - val_loss: 0.7605 - val_acc: 0.7000\n",
      "Epoch 90/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4998 - acc: 0.7895 - val_loss: 0.7463 - val_acc: 0.7000\n",
      "Epoch 91/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4912 - acc: 0.8031 - val_loss: 0.7634 - val_acc: 0.6500\n",
      "Epoch 92/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.4969 - acc: 0.7836 - val_loss: 0.7239 - val_acc: 0.7000\n",
      "Epoch 93/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.5068 - acc: 0.7914 - val_loss: 0.7755 - val_acc: 0.7000\n",
      "Epoch 94/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.4971 - acc: 0.7914 - val_loss: 0.7237 - val_acc: 0.7500\n",
      "Epoch 95/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4834 - acc: 0.7992 - val_loss: 0.7222 - val_acc: 0.7000\n",
      "Epoch 96/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4994 - acc: 0.8148 - val_loss: 0.7063 - val_acc: 0.7500\n",
      "Epoch 97/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.4914 - acc: 0.7992 - val_loss: 0.7548 - val_acc: 0.7000\n",
      "Epoch 98/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.4784 - acc: 0.7992 - val_loss: 0.7034 - val_acc: 0.7000\n",
      "Epoch 99/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.4779 - acc: 0.8090 - val_loss: 0.7005 - val_acc: 0.7000\n",
      "Epoch 100/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.4830 - acc: 0.8148 - val_loss: 0.7365 - val_acc: 0.7500\n",
      "Epoch 101/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.4765 - acc: 0.8012 - val_loss: 0.7247 - val_acc: 0.7000\n",
      "Epoch 102/500\n",
      "513/513 [==============================] - 0s 67us/step - loss: 0.4772 - acc: 0.8090 - val_loss: 0.7055 - val_acc: 0.7000\n",
      "Epoch 103/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.4809 - acc: 0.7992 - val_loss: 0.7105 - val_acc: 0.7000\n",
      "Epoch 104/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.4845 - acc: 0.8031 - val_loss: 0.7118 - val_acc: 0.7000\n",
      "Epoch 105/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4748 - acc: 0.8109 - val_loss: 0.7201 - val_acc: 0.7000\n",
      "Epoch 106/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.4780 - acc: 0.7953 - val_loss: 0.7027 - val_acc: 0.7000\n",
      "Epoch 107/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.4809 - acc: 0.8012 - val_loss: 0.7014 - val_acc: 0.7000\n",
      "Epoch 108/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.4755 - acc: 0.8012 - val_loss: 0.7020 - val_acc: 0.7000\n",
      "Epoch 109/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4673 - acc: 0.8148 - val_loss: 0.6885 - val_acc: 0.7000\n",
      "Epoch 110/500\n",
      "513/513 [==============================] - 0s 106us/step - loss: 0.4670 - acc: 0.8070 - val_loss: 0.7039 - val_acc: 0.7500\n",
      "Epoch 111/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4851 - acc: 0.8031 - val_loss: 0.7215 - val_acc: 0.7000\n",
      "Epoch 112/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4681 - acc: 0.8051 - val_loss: 0.7213 - val_acc: 0.7000\n",
      "Epoch 113/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.4643 - acc: 0.8148 - val_loss: 0.7131 - val_acc: 0.7500\n",
      "Epoch 114/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.4638 - acc: 0.8129 - val_loss: 0.7303 - val_acc: 0.7500\n",
      "Epoch 115/500\n",
      "513/513 [==============================] - 0s 79us/step - loss: 0.4628 - acc: 0.8031 - val_loss: 0.7078 - val_acc: 0.7000\n",
      "Epoch 116/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.4610 - acc: 0.8109 - val_loss: 0.7088 - val_acc: 0.7000\n",
      "Epoch 117/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.4606 - acc: 0.8129 - val_loss: 0.6838 - val_acc: 0.7000\n",
      "Epoch 118/500\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.4628 - acc: 0.8148 - val_loss: 0.6935 - val_acc: 0.7000\n",
      "Epoch 119/500\n",
      "513/513 [==============================] - 0s 57us/step - loss: 0.4666 - acc: 0.8090 - val_loss: 0.7221 - val_acc: 0.7000\n",
      "Epoch 120/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.4635 - acc: 0.8070 - val_loss: 0.6997 - val_acc: 0.7000\n",
      "Epoch 121/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513/513 [==============================] - 0s 59us/step - loss: 0.4560 - acc: 0.8148 - val_loss: 0.7009 - val_acc: 0.7000\n",
      "Epoch 122/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.4571 - acc: 0.8109 - val_loss: 0.7030 - val_acc: 0.7000\n",
      "Epoch 123/500\n",
      "513/513 [==============================] - 0s 58us/step - loss: 0.4553 - acc: 0.8129 - val_loss: 0.7130 - val_acc: 0.7000\n",
      "Epoch 124/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4576 - acc: 0.8090 - val_loss: 0.6646 - val_acc: 0.7000\n",
      "Epoch 125/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.4702 - acc: 0.8109 - val_loss: 0.6940 - val_acc: 0.7000\n",
      "Epoch 126/500\n",
      "513/513 [==============================] - 0s 60us/step - loss: 0.4556 - acc: 0.8226 - val_loss: 0.7004 - val_acc: 0.7000\n",
      "Epoch 127/500\n",
      "513/513 [==============================] - 0s 58us/step - loss: 0.4560 - acc: 0.8070 - val_loss: 0.6928 - val_acc: 0.7000\n",
      "Epoch 128/500\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.4520 - acc: 0.8051 - val_loss: 0.6772 - val_acc: 0.7000\n",
      "Epoch 129/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.4529 - acc: 0.8226 - val_loss: 0.7251 - val_acc: 0.7500\n",
      "Epoch 130/500\n",
      "513/513 [==============================] - 0s 80us/step - loss: 0.4580 - acc: 0.8148 - val_loss: 0.6898 - val_acc: 0.7000\n",
      "Epoch 131/500\n",
      "513/513 [==============================] - 0s 82us/step - loss: 0.4485 - acc: 0.8129 - val_loss: 0.7450 - val_acc: 0.7500\n",
      "Epoch 132/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4484 - acc: 0.8090 - val_loss: 0.6746 - val_acc: 0.7000\n",
      "Epoch 133/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.4449 - acc: 0.8207 - val_loss: 0.6857 - val_acc: 0.7000\n",
      "Epoch 134/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.4684 - acc: 0.8265 - val_loss: 0.6909 - val_acc: 0.7000\n",
      "Epoch 135/500\n",
      "513/513 [==============================] - 0s 63us/step - loss: 0.4603 - acc: 0.8265 - val_loss: 0.7205 - val_acc: 0.7000\n",
      "Epoch 136/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.4615 - acc: 0.8148 - val_loss: 0.7135 - val_acc: 0.7500\n",
      "Epoch 137/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4698 - acc: 0.7934 - val_loss: 0.7290 - val_acc: 0.7000\n",
      "Epoch 138/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.4462 - acc: 0.8265 - val_loss: 0.6908 - val_acc: 0.7000\n",
      "Epoch 139/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.4549 - acc: 0.8246 - val_loss: 0.6893 - val_acc: 0.7000\n",
      "Epoch 140/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.4536 - acc: 0.8246 - val_loss: 0.7158 - val_acc: 0.7000\n",
      "Epoch 141/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4405 - acc: 0.8109 - val_loss: 0.6982 - val_acc: 0.7000\n",
      "Epoch 142/500\n",
      "513/513 [==============================] - 0s 67us/step - loss: 0.4438 - acc: 0.8207 - val_loss: 0.7087 - val_acc: 0.7000\n",
      "Epoch 143/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.4447 - acc: 0.8051 - val_loss: 0.6992 - val_acc: 0.7000\n",
      "Epoch 144/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4459 - acc: 0.8246 - val_loss: 0.6887 - val_acc: 0.7000\n",
      "Epoch 145/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.4414 - acc: 0.8168 - val_loss: 0.6971 - val_acc: 0.7000\n",
      "Epoch 146/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.4393 - acc: 0.8265 - val_loss: 0.7116 - val_acc: 0.7000\n",
      "Epoch 147/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.4379 - acc: 0.8148 - val_loss: 0.7057 - val_acc: 0.7000\n",
      "Epoch 148/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.4383 - acc: 0.8129 - val_loss: 0.7081 - val_acc: 0.7000\n",
      "Epoch 149/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.4354 - acc: 0.8187 - val_loss: 0.6903 - val_acc: 0.7000\n",
      "Epoch 150/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.4363 - acc: 0.8324 - val_loss: 0.7104 - val_acc: 0.7000\n",
      "Epoch 151/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.4325 - acc: 0.8207 - val_loss: 0.7030 - val_acc: 0.7000\n",
      "Epoch 152/500\n",
      "513/513 [==============================] - 0s 63us/step - loss: 0.4331 - acc: 0.8226 - val_loss: 0.7018 - val_acc: 0.7000\n",
      "Epoch 153/500\n",
      "513/513 [==============================] - 0s 79us/step - loss: 0.4391 - acc: 0.8187 - val_loss: 0.7011 - val_acc: 0.7000\n",
      "Epoch 154/500\n",
      "513/513 [==============================] - 0s 88us/step - loss: 0.4316 - acc: 0.8187 - val_loss: 0.6730 - val_acc: 0.7000\n",
      "Epoch 155/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.4328 - acc: 0.8207 - val_loss: 0.6892 - val_acc: 0.7000\n",
      "Epoch 156/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4307 - acc: 0.8285 - val_loss: 0.7000 - val_acc: 0.7000\n",
      "Epoch 157/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.4364 - acc: 0.8129 - val_loss: 0.7222 - val_acc: 0.7000\n",
      "Epoch 158/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.4392 - acc: 0.8090 - val_loss: 0.6926 - val_acc: 0.7000\n",
      "Epoch 159/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.4272 - acc: 0.8246 - val_loss: 0.6845 - val_acc: 0.7000\n",
      "Epoch 160/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.4296 - acc: 0.8265 - val_loss: 0.7147 - val_acc: 0.7000\n",
      "Epoch 161/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.4292 - acc: 0.8168 - val_loss: 0.6975 - val_acc: 0.7500\n",
      "Epoch 162/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.4285 - acc: 0.8304 - val_loss: 0.6868 - val_acc: 0.7000\n",
      "Epoch 163/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.4272 - acc: 0.8304 - val_loss: 0.6946 - val_acc: 0.7000\n",
      "Epoch 164/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.4266 - acc: 0.8207 - val_loss: 0.6665 - val_acc: 0.7000\n",
      "Epoch 165/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.4271 - acc: 0.8226 - val_loss: 0.7022 - val_acc: 0.7000\n",
      "Epoch 166/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.4257 - acc: 0.8187 - val_loss: 0.7084 - val_acc: 0.7000\n",
      "Epoch 167/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4254 - acc: 0.8207 - val_loss: 0.7028 - val_acc: 0.7000\n",
      "Epoch 168/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.4254 - acc: 0.8285 - val_loss: 0.7025 - val_acc: 0.7000\n",
      "Epoch 169/500\n",
      "513/513 [==============================] - 0s 78us/step - loss: 0.4203 - acc: 0.8265 - val_loss: 0.6919 - val_acc: 0.7000\n",
      "Epoch 170/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.4199 - acc: 0.8304 - val_loss: 0.7315 - val_acc: 0.7000\n",
      "Epoch 171/500\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.4264 - acc: 0.8285 - val_loss: 0.7397 - val_acc: 0.7000\n",
      "Epoch 172/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4249 - acc: 0.8187 - val_loss: 0.6721 - val_acc: 0.7500\n",
      "Epoch 173/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.4472 - acc: 0.8265 - val_loss: 0.7431 - val_acc: 0.7000\n",
      "Epoch 174/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4443 - acc: 0.8031 - val_loss: 0.6833 - val_acc: 0.7500\n",
      "Epoch 175/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4567 - acc: 0.8051 - val_loss: 0.7467 - val_acc: 0.7000\n",
      "Epoch 176/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.4380 - acc: 0.8109 - val_loss: 0.7274 - val_acc: 0.7000\n",
      "Epoch 177/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.4555 - acc: 0.7914 - val_loss: 0.6377 - val_acc: 0.7000\n",
      "Epoch 178/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.4321 - acc: 0.8441 - val_loss: 0.7272 - val_acc: 0.7500\n",
      "Epoch 179/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.4239 - acc: 0.8246 - val_loss: 0.7034 - val_acc: 0.7000\n",
      "Epoch 180/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.4221 - acc: 0.8246 - val_loss: 0.6730 - val_acc: 0.7000\n",
      "Epoch 181/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513/513 [==============================] - 0s 69us/step - loss: 0.4142 - acc: 0.8324 - val_loss: 0.6942 - val_acc: 0.7500\n",
      "Epoch 182/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.4141 - acc: 0.8304 - val_loss: 0.6910 - val_acc: 0.7000\n",
      "Epoch 183/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4145 - acc: 0.8402 - val_loss: 0.6799 - val_acc: 0.7500\n",
      "Epoch 184/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4110 - acc: 0.8363 - val_loss: 0.6516 - val_acc: 0.7500\n",
      "Epoch 185/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4254 - acc: 0.8343 - val_loss: 0.6714 - val_acc: 0.7500\n",
      "Epoch 186/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4105 - acc: 0.8343 - val_loss: 0.7034 - val_acc: 0.7500\n",
      "Epoch 187/500\n",
      "513/513 [==============================] - 0s 77us/step - loss: 0.4104 - acc: 0.8343 - val_loss: 0.6737 - val_acc: 0.7500\n",
      "Epoch 188/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.4094 - acc: 0.8363 - val_loss: 0.6777 - val_acc: 0.7000\n",
      "Epoch 189/500\n",
      "513/513 [==============================] - 0s 55us/step - loss: 0.4109 - acc: 0.8363 - val_loss: 0.6627 - val_acc: 0.7500\n",
      "Epoch 190/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.4240 - acc: 0.8226 - val_loss: 0.6782 - val_acc: 0.7000\n",
      "Epoch 191/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4149 - acc: 0.8304 - val_loss: 0.6969 - val_acc: 0.7500\n",
      "Epoch 192/500\n",
      "513/513 [==============================] - 0s 63us/step - loss: 0.4088 - acc: 0.8343 - val_loss: 0.7056 - val_acc: 0.7500\n",
      "Epoch 193/500\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.4160 - acc: 0.8343 - val_loss: 0.6928 - val_acc: 0.7500\n",
      "Epoch 194/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4270 - acc: 0.8343 - val_loss: 0.6904 - val_acc: 0.7000\n",
      "Epoch 195/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4148 - acc: 0.8285 - val_loss: 0.6645 - val_acc: 0.7500\n",
      "Epoch 196/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.4101 - acc: 0.8441 - val_loss: 0.6983 - val_acc: 0.7000\n",
      "Epoch 197/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.4061 - acc: 0.8382 - val_loss: 0.6563 - val_acc: 0.7500\n",
      "Epoch 198/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4059 - acc: 0.8402 - val_loss: 0.6916 - val_acc: 0.7500\n",
      "Epoch 199/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.4060 - acc: 0.8441 - val_loss: 0.6810 - val_acc: 0.7500\n",
      "Epoch 200/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4043 - acc: 0.8304 - val_loss: 0.6814 - val_acc: 0.7500\n",
      "Epoch 201/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.4288 - acc: 0.8382 - val_loss: 0.6643 - val_acc: 0.7500\n",
      "Epoch 202/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4083 - acc: 0.8324 - val_loss: 0.6939 - val_acc: 0.7000\n",
      "Epoch 203/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.4015 - acc: 0.8402 - val_loss: 0.6922 - val_acc: 0.7500\n",
      "Epoch 204/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.4021 - acc: 0.8382 - val_loss: 0.6813 - val_acc: 0.7500\n",
      "Epoch 205/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4149 - acc: 0.8382 - val_loss: 0.7211 - val_acc: 0.7000\n",
      "Epoch 206/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.4123 - acc: 0.8363 - val_loss: 0.6742 - val_acc: 0.7000\n",
      "Epoch 207/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.4018 - acc: 0.8402 - val_loss: 0.7009 - val_acc: 0.7500\n",
      "Epoch 208/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3989 - acc: 0.8402 - val_loss: 0.6714 - val_acc: 0.7500\n",
      "Epoch 209/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.4052 - acc: 0.8480 - val_loss: 0.6605 - val_acc: 0.7000\n",
      "Epoch 210/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.4143 - acc: 0.8402 - val_loss: 0.7216 - val_acc: 0.7000\n",
      "Epoch 211/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.4016 - acc: 0.8460 - val_loss: 0.7042 - val_acc: 0.7500\n",
      "Epoch 212/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.4052 - acc: 0.8285 - val_loss: 0.7275 - val_acc: 0.7000\n",
      "Epoch 213/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.4110 - acc: 0.8246 - val_loss: 0.7286 - val_acc: 0.7500\n",
      "Epoch 214/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.3970 - acc: 0.8402 - val_loss: 0.6297 - val_acc: 0.7500\n",
      "Epoch 215/500\n",
      "513/513 [==============================] - 0s 77us/step - loss: 0.4365 - acc: 0.8285 - val_loss: 0.7734 - val_acc: 0.7000\n",
      "Epoch 216/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.4446 - acc: 0.7934 - val_loss: 0.7020 - val_acc: 0.7000\n",
      "Epoch 217/500\n",
      "513/513 [==============================] - 0s 58us/step - loss: 0.4040 - acc: 0.8596 - val_loss: 0.6775 - val_acc: 0.7500\n",
      "Epoch 218/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.3945 - acc: 0.8441 - val_loss: 0.6787 - val_acc: 0.7000\n",
      "Epoch 219/500\n",
      "513/513 [==============================] - 0s 82us/step - loss: 0.3955 - acc: 0.8558 - val_loss: 0.6712 - val_acc: 0.7500\n",
      "Epoch 220/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3925 - acc: 0.8499 - val_loss: 0.7002 - val_acc: 0.7500\n",
      "Epoch 221/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.3939 - acc: 0.8421 - val_loss: 0.6623 - val_acc: 0.7500\n",
      "Epoch 222/500\n",
      "513/513 [==============================] - 0s 81us/step - loss: 0.4096 - acc: 0.8402 - val_loss: 0.6502 - val_acc: 0.7500\n",
      "Epoch 223/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.4052 - acc: 0.8441 - val_loss: 0.6557 - val_acc: 0.7500\n",
      "Epoch 224/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.3938 - acc: 0.8460 - val_loss: 0.6419 - val_acc: 0.7500\n",
      "Epoch 225/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.4370 - acc: 0.8441 - val_loss: 0.7224 - val_acc: 0.7500\n",
      "Epoch 226/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.4118 - acc: 0.8324 - val_loss: 0.7370 - val_acc: 0.7500\n",
      "Epoch 227/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3943 - acc: 0.8382 - val_loss: 0.6874 - val_acc: 0.7500\n",
      "Epoch 228/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3918 - acc: 0.8499 - val_loss: 0.7286 - val_acc: 0.7500\n",
      "Epoch 229/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3946 - acc: 0.8480 - val_loss: 0.6421 - val_acc: 0.7500\n",
      "Epoch 230/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4280 - acc: 0.8343 - val_loss: 0.7210 - val_acc: 0.7000\n",
      "Epoch 231/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3969 - acc: 0.8343 - val_loss: 0.7087 - val_acc: 0.7500\n",
      "Epoch 232/500\n",
      "513/513 [==============================] - 0s 87us/step - loss: 0.3922 - acc: 0.8577 - val_loss: 0.6922 - val_acc: 0.7500\n",
      "Epoch 233/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.3898 - acc: 0.8480 - val_loss: 0.6928 - val_acc: 0.7500\n",
      "Epoch 234/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3890 - acc: 0.8402 - val_loss: 0.6330 - val_acc: 0.7500\n",
      "Epoch 235/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.4114 - acc: 0.8499 - val_loss: 0.7104 - val_acc: 0.7500\n",
      "Epoch 236/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3910 - acc: 0.8460 - val_loss: 0.7149 - val_acc: 0.7000\n",
      "Epoch 237/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.4242 - acc: 0.8070 - val_loss: 0.6706 - val_acc: 0.7000\n",
      "Epoch 238/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.4089 - acc: 0.8402 - val_loss: 0.6965 - val_acc: 0.7500\n",
      "Epoch 239/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3913 - acc: 0.8499 - val_loss: 0.6838 - val_acc: 0.7500\n",
      "Epoch 240/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3859 - acc: 0.8480 - val_loss: 0.6625 - val_acc: 0.7500\n",
      "Epoch 241/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513/513 [==============================] - 0s 63us/step - loss: 0.3948 - acc: 0.8519 - val_loss: 0.6856 - val_acc: 0.7500\n",
      "Epoch 242/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.3980 - acc: 0.8480 - val_loss: 0.6812 - val_acc: 0.7500\n",
      "Epoch 243/500\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.3895 - acc: 0.8499 - val_loss: 0.6779 - val_acc: 0.7500\n",
      "Epoch 244/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.3980 - acc: 0.8460 - val_loss: 0.7286 - val_acc: 0.7500\n",
      "Epoch 245/500\n",
      "513/513 [==============================] - 0s 78us/step - loss: 0.3849 - acc: 0.8519 - val_loss: 0.7007 - val_acc: 0.7500\n",
      "Epoch 246/500\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.3908 - acc: 0.8480 - val_loss: 0.6892 - val_acc: 0.7500\n",
      "Epoch 247/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3863 - acc: 0.8655 - val_loss: 0.6743 - val_acc: 0.7500\n",
      "Epoch 248/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3823 - acc: 0.8480 - val_loss: 0.7310 - val_acc: 0.7000\n",
      "Epoch 249/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3871 - acc: 0.8538 - val_loss: 0.6862 - val_acc: 0.7500\n",
      "Epoch 250/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3818 - acc: 0.8558 - val_loss: 0.6745 - val_acc: 0.7500\n",
      "Epoch 251/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3843 - acc: 0.8460 - val_loss: 0.6807 - val_acc: 0.7500\n",
      "Epoch 252/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3803 - acc: 0.8538 - val_loss: 0.6722 - val_acc: 0.7500\n",
      "Epoch 253/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3829 - acc: 0.8577 - val_loss: 0.7273 - val_acc: 0.7500\n",
      "Epoch 254/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.3843 - acc: 0.8460 - val_loss: 0.6990 - val_acc: 0.7500\n",
      "Epoch 255/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3818 - acc: 0.8499 - val_loss: 0.6546 - val_acc: 0.7500\n",
      "Epoch 256/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.4119 - acc: 0.8421 - val_loss: 0.7252 - val_acc: 0.7000\n",
      "Epoch 257/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3882 - acc: 0.8343 - val_loss: 0.6826 - val_acc: 0.7500\n",
      "Epoch 258/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3968 - acc: 0.8285 - val_loss: 0.7009 - val_acc: 0.7500\n",
      "Epoch 259/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3771 - acc: 0.8499 - val_loss: 0.6921 - val_acc: 0.7500\n",
      "Epoch 260/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3842 - acc: 0.8441 - val_loss: 0.6986 - val_acc: 0.7000\n",
      "Epoch 261/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.4007 - acc: 0.8324 - val_loss: 0.6459 - val_acc: 0.7500\n",
      "Epoch 262/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3923 - acc: 0.8655 - val_loss: 0.7032 - val_acc: 0.7500\n",
      "Epoch 263/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.3813 - acc: 0.8441 - val_loss: 0.6645 - val_acc: 0.7500\n",
      "Epoch 264/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.4180 - acc: 0.8441 - val_loss: 0.6705 - val_acc: 0.7000\n",
      "Epoch 265/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3895 - acc: 0.8285 - val_loss: 0.7053 - val_acc: 0.7000\n",
      "Epoch 266/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.3764 - acc: 0.8635 - val_loss: 0.7209 - val_acc: 0.7500\n",
      "Epoch 267/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.3765 - acc: 0.8519 - val_loss: 0.7513 - val_acc: 0.7500\n",
      "Epoch 268/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.3979 - acc: 0.8304 - val_loss: 0.6866 - val_acc: 0.7000\n",
      "Epoch 269/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.3747 - acc: 0.8635 - val_loss: 0.6895 - val_acc: 0.7500\n",
      "Epoch 270/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3843 - acc: 0.8577 - val_loss: 0.7267 - val_acc: 0.7000\n",
      "Epoch 271/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.3886 - acc: 0.8246 - val_loss: 0.7003 - val_acc: 0.7500\n",
      "Epoch 272/500\n",
      "513/513 [==============================] - 0s 79us/step - loss: 0.3744 - acc: 0.8655 - val_loss: 0.7076 - val_acc: 0.7500\n",
      "Epoch 273/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.3917 - acc: 0.8421 - val_loss: 0.7368 - val_acc: 0.7500\n",
      "Epoch 274/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.3940 - acc: 0.8460 - val_loss: 0.7048 - val_acc: 0.7000\n",
      "Epoch 275/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3974 - acc: 0.8226 - val_loss: 0.6780 - val_acc: 0.7000\n",
      "Epoch 276/500\n",
      "513/513 [==============================] - 0s 81us/step - loss: 0.3850 - acc: 0.8616 - val_loss: 0.7210 - val_acc: 0.7500\n",
      "Epoch 277/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3930 - acc: 0.8304 - val_loss: 0.7053 - val_acc: 0.7000\n",
      "Epoch 278/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3720 - acc: 0.8635 - val_loss: 0.7006 - val_acc: 0.7500\n",
      "Epoch 279/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3809 - acc: 0.8421 - val_loss: 0.7027 - val_acc: 0.7500\n",
      "Epoch 280/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3727 - acc: 0.8596 - val_loss: 0.6775 - val_acc: 0.7500\n",
      "Epoch 281/500\n",
      "513/513 [==============================] - 0s 63us/step - loss: 0.3708 - acc: 0.8577 - val_loss: 0.6843 - val_acc: 0.7500\n",
      "Epoch 282/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3827 - acc: 0.8577 - val_loss: 0.7158 - val_acc: 0.7500\n",
      "Epoch 283/500\n",
      "513/513 [==============================] - 0s 63us/step - loss: 0.3740 - acc: 0.8596 - val_loss: 0.6700 - val_acc: 0.7500\n",
      "Epoch 284/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3767 - acc: 0.8558 - val_loss: 0.7270 - val_acc: 0.7000\n",
      "Epoch 285/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.3721 - acc: 0.8499 - val_loss: 0.6668 - val_acc: 0.7500\n",
      "Epoch 286/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3697 - acc: 0.8635 - val_loss: 0.7372 - val_acc: 0.7000\n",
      "Epoch 287/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3886 - acc: 0.8324 - val_loss: 0.6638 - val_acc: 0.7500\n",
      "Epoch 288/500\n",
      "513/513 [==============================] - 0s 77us/step - loss: 0.3926 - acc: 0.8577 - val_loss: 0.7185 - val_acc: 0.7500\n",
      "Epoch 289/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3900 - acc: 0.8207 - val_loss: 0.6910 - val_acc: 0.7500\n",
      "Epoch 290/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3750 - acc: 0.8674 - val_loss: 0.6799 - val_acc: 0.7500\n",
      "Epoch 291/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.4049 - acc: 0.8441 - val_loss: 0.7110 - val_acc: 0.7500\n",
      "Epoch 292/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3824 - acc: 0.8304 - val_loss: 0.6718 - val_acc: 0.7500\n",
      "Epoch 293/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.3976 - acc: 0.8713 - val_loss: 0.7112 - val_acc: 0.7500\n",
      "Epoch 294/500\n",
      "513/513 [==============================] - 0s 77us/step - loss: 0.3724 - acc: 0.8480 - val_loss: 0.6774 - val_acc: 0.7500\n",
      "Epoch 295/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3832 - acc: 0.8538 - val_loss: 0.7211 - val_acc: 0.7500\n",
      "Epoch 296/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3688 - acc: 0.8460 - val_loss: 0.6855 - val_acc: 0.7500\n",
      "Epoch 297/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.3724 - acc: 0.8577 - val_loss: 0.7187 - val_acc: 0.7500\n",
      "Epoch 298/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3941 - acc: 0.8265 - val_loss: 0.7028 - val_acc: 0.7500\n",
      "Epoch 299/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.3710 - acc: 0.8635 - val_loss: 0.6890 - val_acc: 0.7500\n",
      "Epoch 300/500\n",
      "513/513 [==============================] - 0s 90us/step - loss: 0.3662 - acc: 0.8519 - val_loss: 0.7042 - val_acc: 0.7500\n",
      "Epoch 301/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513/513 [==============================] - 0s 81us/step - loss: 0.3640 - acc: 0.8538 - val_loss: 0.7078 - val_acc: 0.7500\n",
      "Epoch 302/500\n",
      "513/513 [==============================] - 0s 83us/step - loss: 0.3630 - acc: 0.8577 - val_loss: 0.6869 - val_acc: 0.7500\n",
      "Epoch 303/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.3621 - acc: 0.8655 - val_loss: 0.7113 - val_acc: 0.7500\n",
      "Epoch 304/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3651 - acc: 0.8538 - val_loss: 0.6788 - val_acc: 0.7500\n",
      "Epoch 305/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.3663 - acc: 0.8733 - val_loss: 0.7054 - val_acc: 0.7500\n",
      "Epoch 306/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3623 - acc: 0.8577 - val_loss: 0.6909 - val_acc: 0.7500\n",
      "Epoch 307/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3602 - acc: 0.8596 - val_loss: 0.7259 - val_acc: 0.7500\n",
      "Epoch 308/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3643 - acc: 0.8499 - val_loss: 0.6744 - val_acc: 0.7500\n",
      "Epoch 309/500\n",
      "513/513 [==============================] - 0s 67us/step - loss: 0.3633 - acc: 0.8635 - val_loss: 0.7355 - val_acc: 0.7500\n",
      "Epoch 310/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3609 - acc: 0.8558 - val_loss: 0.6937 - val_acc: 0.7500\n",
      "Epoch 311/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.3600 - acc: 0.8674 - val_loss: 0.7279 - val_acc: 0.7500\n",
      "Epoch 312/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3620 - acc: 0.8480 - val_loss: 0.6995 - val_acc: 0.7500\n",
      "Epoch 313/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.3626 - acc: 0.8635 - val_loss: 0.6976 - val_acc: 0.7500\n",
      "Epoch 314/500\n",
      "513/513 [==============================] - 0s 82us/step - loss: 0.3610 - acc: 0.8616 - val_loss: 0.7184 - val_acc: 0.7500\n",
      "Epoch 315/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3579 - acc: 0.8577 - val_loss: 0.7001 - val_acc: 0.7500\n",
      "Epoch 316/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3581 - acc: 0.8635 - val_loss: 0.6776 - val_acc: 0.7500\n",
      "Epoch 317/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3591 - acc: 0.8674 - val_loss: 0.6968 - val_acc: 0.7500\n",
      "Epoch 318/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3578 - acc: 0.8577 - val_loss: 0.6916 - val_acc: 0.7500\n",
      "Epoch 319/500\n",
      "513/513 [==============================] - 0s 78us/step - loss: 0.3604 - acc: 0.8635 - val_loss: 0.7707 - val_acc: 0.7500\n",
      "Epoch 320/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.3692 - acc: 0.8499 - val_loss: 0.6731 - val_acc: 0.7500\n",
      "Epoch 321/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.3544 - acc: 0.8655 - val_loss: 0.6965 - val_acc: 0.7500\n",
      "Epoch 322/500\n",
      "513/513 [==============================] - 0s 82us/step - loss: 0.3627 - acc: 0.8616 - val_loss: 0.7451 - val_acc: 0.7500\n",
      "Epoch 323/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3679 - acc: 0.8460 - val_loss: 0.6834 - val_acc: 0.7500\n",
      "Epoch 324/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3685 - acc: 0.8635 - val_loss: 0.7086 - val_acc: 0.7500\n",
      "Epoch 325/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3560 - acc: 0.8635 - val_loss: 0.7163 - val_acc: 0.7500\n",
      "Epoch 326/500\n",
      "513/513 [==============================] - 0s 57us/step - loss: 0.3604 - acc: 0.8519 - val_loss: 0.6599 - val_acc: 0.7500\n",
      "Epoch 327/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3542 - acc: 0.8733 - val_loss: 0.7598 - val_acc: 0.7500\n",
      "Epoch 328/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.3695 - acc: 0.8402 - val_loss: 0.6685 - val_acc: 0.7500\n",
      "Epoch 329/500\n",
      "513/513 [==============================] - 0s 83us/step - loss: 0.3671 - acc: 0.8519 - val_loss: 0.7121 - val_acc: 0.7500\n",
      "Epoch 330/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3557 - acc: 0.8519 - val_loss: 0.7142 - val_acc: 0.7500\n",
      "Epoch 331/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3557 - acc: 0.8577 - val_loss: 0.7089 - val_acc: 0.7500\n",
      "Epoch 332/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3545 - acc: 0.8519 - val_loss: 0.7047 - val_acc: 0.7500\n",
      "Epoch 333/500\n",
      "513/513 [==============================] - 0s 63us/step - loss: 0.3591 - acc: 0.8596 - val_loss: 0.7008 - val_acc: 0.7500\n",
      "Epoch 334/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3550 - acc: 0.8558 - val_loss: 0.7061 - val_acc: 0.7500\n",
      "Epoch 335/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.3672 - acc: 0.8538 - val_loss: 0.6909 - val_acc: 0.7500\n",
      "Epoch 336/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3506 - acc: 0.8772 - val_loss: 0.6629 - val_acc: 0.7500\n",
      "Epoch 337/500\n",
      "513/513 [==============================] - 0s 81us/step - loss: 0.3630 - acc: 0.8480 - val_loss: 0.6172 - val_acc: 0.7000\n",
      "Epoch 338/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.4001 - acc: 0.8499 - val_loss: 0.7798 - val_acc: 0.7500\n",
      "Epoch 339/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3734 - acc: 0.8441 - val_loss: 0.6857 - val_acc: 0.7500\n",
      "Epoch 340/500\n",
      "513/513 [==============================] - 0s 83us/step - loss: 0.3579 - acc: 0.8655 - val_loss: 0.6746 - val_acc: 0.7500\n",
      "Epoch 341/500\n",
      "513/513 [==============================] - 0s 79us/step - loss: 0.3555 - acc: 0.8713 - val_loss: 0.7236 - val_acc: 0.7500\n",
      "Epoch 342/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3524 - acc: 0.8577 - val_loss: 0.6891 - val_acc: 0.7500\n",
      "Epoch 343/500\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.3506 - acc: 0.8694 - val_loss: 0.6698 - val_acc: 0.7500\n",
      "Epoch 344/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3534 - acc: 0.8713 - val_loss: 0.7143 - val_acc: 0.7500\n",
      "Epoch 345/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3507 - acc: 0.8538 - val_loss: 0.6952 - val_acc: 0.7500\n",
      "Epoch 346/500\n",
      "513/513 [==============================] - 0s 88us/step - loss: 0.3532 - acc: 0.8655 - val_loss: 0.7128 - val_acc: 0.7000\n",
      "Epoch 347/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.3616 - acc: 0.8480 - val_loss: 0.6817 - val_acc: 0.7500\n",
      "Epoch 348/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3472 - acc: 0.8752 - val_loss: 0.7127 - val_acc: 0.7500\n",
      "Epoch 349/500\n",
      "513/513 [==============================] - 0s 78us/step - loss: 0.3450 - acc: 0.8616 - val_loss: 0.7031 - val_acc: 0.7500\n",
      "Epoch 350/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3517 - acc: 0.8596 - val_loss: 0.7023 - val_acc: 0.7500\n",
      "Epoch 351/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3468 - acc: 0.8635 - val_loss: 0.6940 - val_acc: 0.7500\n",
      "Epoch 352/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3441 - acc: 0.8674 - val_loss: 0.6856 - val_acc: 0.7500\n",
      "Epoch 353/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.3444 - acc: 0.8558 - val_loss: 0.6856 - val_acc: 0.7500\n",
      "Epoch 354/500\n",
      "513/513 [==============================] - 0s 84us/step - loss: 0.3611 - acc: 0.8655 - val_loss: 0.7203 - val_acc: 0.8000\n",
      "Epoch 355/500\n",
      "513/513 [==============================] - 0s 79us/step - loss: 0.3515 - acc: 0.8499 - val_loss: 0.6575 - val_acc: 0.7000\n",
      "Epoch 356/500\n",
      "513/513 [==============================] - 0s 78us/step - loss: 0.3462 - acc: 0.8655 - val_loss: 0.7136 - val_acc: 0.7500\n",
      "Epoch 357/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.3482 - acc: 0.8713 - val_loss: 0.7019 - val_acc: 0.7500\n",
      "Epoch 358/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3950 - acc: 0.8207 - val_loss: 0.6783 - val_acc: 0.7000\n",
      "Epoch 359/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.3492 - acc: 0.8752 - val_loss: 0.7006 - val_acc: 0.7500\n",
      "Epoch 360/500\n",
      "513/513 [==============================] - 0s 63us/step - loss: 0.3814 - acc: 0.8616 - val_loss: 0.6952 - val_acc: 0.7500\n",
      "Epoch 361/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513/513 [==============================] - 0s 62us/step - loss: 0.3468 - acc: 0.8694 - val_loss: 0.6789 - val_acc: 0.7000\n",
      "Epoch 362/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.3423 - acc: 0.8655 - val_loss: 0.7028 - val_acc: 0.7500\n",
      "Epoch 363/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3409 - acc: 0.8713 - val_loss: 0.7306 - val_acc: 0.7500\n",
      "Epoch 364/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.3430 - acc: 0.8635 - val_loss: 0.6801 - val_acc: 0.7000\n",
      "Epoch 365/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3416 - acc: 0.8811 - val_loss: 0.7364 - val_acc: 0.7500\n",
      "Epoch 366/500\n",
      "513/513 [==============================] - 0s 63us/step - loss: 0.3452 - acc: 0.8558 - val_loss: 0.6882 - val_acc: 0.7000\n",
      "Epoch 367/500\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.3418 - acc: 0.8733 - val_loss: 0.7508 - val_acc: 0.7500\n",
      "Epoch 368/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3521 - acc: 0.8499 - val_loss: 0.6947 - val_acc: 0.7000\n",
      "Epoch 369/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3479 - acc: 0.8674 - val_loss: 0.6653 - val_acc: 0.7000\n",
      "Epoch 370/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3683 - acc: 0.8558 - val_loss: 0.7613 - val_acc: 0.7500\n",
      "Epoch 371/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3447 - acc: 0.8635 - val_loss: 0.7012 - val_acc: 0.7500\n",
      "Epoch 372/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3438 - acc: 0.8538 - val_loss: 0.7256 - val_acc: 0.7500\n",
      "Epoch 373/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.3395 - acc: 0.8713 - val_loss: 0.6963 - val_acc: 0.7000\n",
      "Epoch 374/500\n",
      "513/513 [==============================] - 0s 63us/step - loss: 0.3404 - acc: 0.8733 - val_loss: 0.6656 - val_acc: 0.7000\n",
      "Epoch 375/500\n",
      "513/513 [==============================] - 0s 84us/step - loss: 0.4048 - acc: 0.8558 - val_loss: 0.7137 - val_acc: 0.7500\n",
      "Epoch 376/500\n",
      "513/513 [==============================] - 0s 79us/step - loss: 0.3424 - acc: 0.8772 - val_loss: 0.6988 - val_acc: 0.7500\n",
      "Epoch 377/500\n",
      "513/513 [==============================] - 0s 125us/step - loss: 0.3421 - acc: 0.8655 - val_loss: 0.7197 - val_acc: 0.7500\n",
      "Epoch 378/500\n",
      "513/513 [==============================] - 0s 89us/step - loss: 0.3453 - acc: 0.8635 - val_loss: 0.7076 - val_acc: 0.7500\n",
      "Epoch 379/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.3379 - acc: 0.8616 - val_loss: 0.7141 - val_acc: 0.7500\n",
      "Epoch 380/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.3389 - acc: 0.8655 - val_loss: 0.7445 - val_acc: 0.7500\n",
      "Epoch 381/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3365 - acc: 0.8655 - val_loss: 0.7144 - val_acc: 0.7500\n",
      "Epoch 382/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3356 - acc: 0.8752 - val_loss: 0.6921 - val_acc: 0.7000\n",
      "Epoch 383/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3356 - acc: 0.8772 - val_loss: 0.7429 - val_acc: 0.7500\n",
      "Epoch 384/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3353 - acc: 0.8616 - val_loss: 0.7080 - val_acc: 0.7000\n",
      "Epoch 385/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3727 - acc: 0.8538 - val_loss: 0.7452 - val_acc: 0.7500\n",
      "Epoch 386/500\n",
      "513/513 [==============================] - 0s 63us/step - loss: 0.3511 - acc: 0.8674 - val_loss: 0.7272 - val_acc: 0.7500\n",
      "Epoch 387/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3374 - acc: 0.8655 - val_loss: 0.7169 - val_acc: 0.7500\n",
      "Epoch 388/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3366 - acc: 0.8733 - val_loss: 0.7253 - val_acc: 0.7500\n",
      "Epoch 389/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3331 - acc: 0.8674 - val_loss: 0.7139 - val_acc: 0.7500\n",
      "Epoch 390/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3366 - acc: 0.8635 - val_loss: 0.7117 - val_acc: 0.7500\n",
      "Epoch 391/500\n",
      "513/513 [==============================] - 0s 67us/step - loss: 0.3352 - acc: 0.8674 - val_loss: 0.7205 - val_acc: 0.7500\n",
      "Epoch 392/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3468 - acc: 0.8499 - val_loss: 0.6917 - val_acc: 0.7000\n",
      "Epoch 393/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3397 - acc: 0.8752 - val_loss: 0.7509 - val_acc: 0.7500\n",
      "Epoch 394/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3365 - acc: 0.8713 - val_loss: 0.7110 - val_acc: 0.7500\n",
      "Epoch 395/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3312 - acc: 0.8694 - val_loss: 0.7058 - val_acc: 0.7500\n",
      "Epoch 396/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.3323 - acc: 0.8752 - val_loss: 0.7112 - val_acc: 0.7500\n",
      "Epoch 397/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.3397 - acc: 0.8596 - val_loss: 0.7198 - val_acc: 0.7500\n",
      "Epoch 398/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.3440 - acc: 0.8499 - val_loss: 0.6913 - val_acc: 0.7000\n",
      "Epoch 399/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3328 - acc: 0.8772 - val_loss: 0.7358 - val_acc: 0.7500\n",
      "Epoch 400/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3343 - acc: 0.8674 - val_loss: 0.6888 - val_acc: 0.7500\n",
      "Epoch 401/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.3301 - acc: 0.8791 - val_loss: 0.7498 - val_acc: 0.7500\n",
      "Epoch 402/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3357 - acc: 0.8674 - val_loss: 0.6719 - val_acc: 0.7000\n",
      "Epoch 403/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.3330 - acc: 0.8889 - val_loss: 0.7337 - val_acc: 0.7500\n",
      "Epoch 404/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3322 - acc: 0.8616 - val_loss: 0.6516 - val_acc: 0.7000\n",
      "Epoch 405/500\n",
      "513/513 [==============================] - 0s 83us/step - loss: 0.3477 - acc: 0.8713 - val_loss: 0.7641 - val_acc: 0.7000\n",
      "Epoch 406/500\n",
      "513/513 [==============================] - 0s 77us/step - loss: 0.3340 - acc: 0.8752 - val_loss: 0.7047 - val_acc: 0.7000\n",
      "Epoch 407/500\n",
      "513/513 [==============================] - 0s 80us/step - loss: 0.3276 - acc: 0.8850 - val_loss: 0.7332 - val_acc: 0.7000\n",
      "Epoch 408/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.3337 - acc: 0.8772 - val_loss: 0.7218 - val_acc: 0.7000\n",
      "Epoch 409/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3303 - acc: 0.8733 - val_loss: 0.7130 - val_acc: 0.7000\n",
      "Epoch 410/500\n",
      "513/513 [==============================] - 0s 78us/step - loss: 0.3320 - acc: 0.8850 - val_loss: 0.7639 - val_acc: 0.7500\n",
      "Epoch 411/500\n",
      "513/513 [==============================] - 0s 67us/step - loss: 0.3333 - acc: 0.8616 - val_loss: 0.7017 - val_acc: 0.7000\n",
      "Epoch 412/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3271 - acc: 0.8752 - val_loss: 0.7331 - val_acc: 0.7500\n",
      "Epoch 413/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3582 - acc: 0.8402 - val_loss: 0.6679 - val_acc: 0.7000\n",
      "Epoch 414/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3439 - acc: 0.8674 - val_loss: 0.7204 - val_acc: 0.7500\n",
      "Epoch 415/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3294 - acc: 0.8674 - val_loss: 0.7493 - val_acc: 0.7500\n",
      "Epoch 416/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.3655 - acc: 0.8480 - val_loss: 0.6729 - val_acc: 0.7000\n",
      "Epoch 417/500\n",
      "513/513 [==============================] - 0s 78us/step - loss: 0.3320 - acc: 0.8655 - val_loss: 0.6885 - val_acc: 0.7000\n",
      "Epoch 418/500\n",
      "513/513 [==============================] - 0s 79us/step - loss: 0.3265 - acc: 0.8752 - val_loss: 0.7047 - val_acc: 0.7000\n",
      "Epoch 419/500\n",
      "513/513 [==============================] - 0s 70us/step - loss: 0.3239 - acc: 0.8713 - val_loss: 0.6602 - val_acc: 0.7000\n",
      "Epoch 420/500\n",
      "513/513 [==============================] - 0s 75us/step - loss: 0.3598 - acc: 0.8558 - val_loss: 0.8352 - val_acc: 0.7500\n",
      "Epoch 421/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513/513 [==============================] - 0s 76us/step - loss: 0.3727 - acc: 0.8558 - val_loss: 0.6589 - val_acc: 0.7000\n",
      "Epoch 422/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.3423 - acc: 0.8772 - val_loss: 0.7350 - val_acc: 0.7500\n",
      "Epoch 423/500\n",
      "513/513 [==============================] - 0s 84us/step - loss: 0.3256 - acc: 0.8674 - val_loss: 0.6787 - val_acc: 0.7000\n",
      "Epoch 424/500\n",
      "513/513 [==============================] - 0s 78us/step - loss: 0.3422 - acc: 0.8850 - val_loss: 0.7568 - val_acc: 0.7500\n",
      "Epoch 425/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.3310 - acc: 0.8694 - val_loss: 0.7347 - val_acc: 0.7500\n",
      "Epoch 426/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3240 - acc: 0.8713 - val_loss: 0.6847 - val_acc: 0.7000\n",
      "Epoch 427/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.3610 - acc: 0.8713 - val_loss: 0.7292 - val_acc: 0.7500\n",
      "Epoch 428/500\n",
      "513/513 [==============================] - 0s 56us/step - loss: 0.3275 - acc: 0.8674 - val_loss: 0.7197 - val_acc: 0.7000\n",
      "Epoch 429/500\n",
      "513/513 [==============================] - 0s 78us/step - loss: 0.4111 - acc: 0.8402 - val_loss: 0.7672 - val_acc: 0.7500\n",
      "Epoch 430/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3323 - acc: 0.8733 - val_loss: 0.7481 - val_acc: 0.8000\n",
      "Epoch 431/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.3226 - acc: 0.8733 - val_loss: 0.7018 - val_acc: 0.7000\n",
      "Epoch 432/500\n",
      "513/513 [==============================] - 0s 57us/step - loss: 0.3345 - acc: 0.8889 - val_loss: 0.7346 - val_acc: 0.7500\n",
      "Epoch 433/500\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.3243 - acc: 0.8811 - val_loss: 0.7440 - val_acc: 0.7500\n",
      "Epoch 434/500\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.3478 - acc: 0.8772 - val_loss: 0.7563 - val_acc: 0.7500\n",
      "Epoch 435/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3249 - acc: 0.8733 - val_loss: 0.6987 - val_acc: 0.7000\n",
      "Epoch 436/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3218 - acc: 0.8889 - val_loss: 0.7440 - val_acc: 0.7500\n",
      "Epoch 437/500\n",
      "513/513 [==============================] - 0s 60us/step - loss: 0.3214 - acc: 0.8752 - val_loss: 0.7146 - val_acc: 0.7000\n",
      "Epoch 438/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3188 - acc: 0.8733 - val_loss: 0.7721 - val_acc: 0.7500\n",
      "Epoch 439/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.4219 - acc: 0.8070 - val_loss: 0.6701 - val_acc: 0.8000\n",
      "Epoch 440/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.3383 - acc: 0.8752 - val_loss: 0.7724 - val_acc: 0.7500\n",
      "Epoch 441/500\n",
      "513/513 [==============================] - 0s 81us/step - loss: 0.3202 - acc: 0.8752 - val_loss: 0.7139 - val_acc: 0.7000\n",
      "Epoch 442/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.3196 - acc: 0.8713 - val_loss: 0.7390 - val_acc: 0.7500\n",
      "Epoch 443/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3172 - acc: 0.8772 - val_loss: 0.6914 - val_acc: 0.7000\n",
      "Epoch 444/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3244 - acc: 0.8850 - val_loss: 0.7359 - val_acc: 0.7500\n",
      "Epoch 445/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3178 - acc: 0.8752 - val_loss: 0.6950 - val_acc: 0.7000\n",
      "Epoch 446/500\n",
      "513/513 [==============================] - 0s 61us/step - loss: 0.3184 - acc: 0.8850 - val_loss: 0.7333 - val_acc: 0.7000\n",
      "Epoch 447/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3159 - acc: 0.8772 - val_loss: 0.7085 - val_acc: 0.7000\n",
      "Epoch 448/500\n",
      "513/513 [==============================] - 0s 87us/step - loss: 0.3187 - acc: 0.8811 - val_loss: 0.7518 - val_acc: 0.7500\n",
      "Epoch 449/500\n",
      "513/513 [==============================] - 0s 77us/step - loss: 0.3156 - acc: 0.8791 - val_loss: 0.6789 - val_acc: 0.7000\n",
      "Epoch 450/500\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.3393 - acc: 0.8850 - val_loss: 0.8080 - val_acc: 0.7500\n",
      "Epoch 451/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3779 - acc: 0.8460 - val_loss: 0.7393 - val_acc: 0.7000\n",
      "Epoch 452/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3246 - acc: 0.8850 - val_loss: 0.7399 - val_acc: 0.7000\n",
      "Epoch 453/500\n",
      "513/513 [==============================] - 0s 58us/step - loss: 0.3349 - acc: 0.8480 - val_loss: 0.6964 - val_acc: 0.7000\n",
      "Epoch 454/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3462 - acc: 0.8733 - val_loss: 0.6796 - val_acc: 0.7500\n",
      "Epoch 455/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.3759 - acc: 0.8635 - val_loss: 0.8477 - val_acc: 0.7500\n",
      "Epoch 456/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.3537 - acc: 0.8499 - val_loss: 0.7381 - val_acc: 0.7500\n",
      "Epoch 457/500\n",
      "513/513 [==============================] - 0s 87us/step - loss: 0.3214 - acc: 0.8850 - val_loss: 0.7314 - val_acc: 0.7500\n",
      "Epoch 458/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3422 - acc: 0.8499 - val_loss: 0.7249 - val_acc: 0.7000\n",
      "Epoch 459/500\n",
      "513/513 [==============================] - 0s 76us/step - loss: 0.3188 - acc: 0.8928 - val_loss: 0.7345 - val_acc: 0.7000\n",
      "Epoch 460/500\n",
      "513/513 [==============================] - ETA: 0s - loss: 0.4575 - acc: 0.906 - 0s 70us/step - loss: 0.3142 - acc: 0.8850 - val_loss: 0.7319 - val_acc: 0.7000\n",
      "Epoch 461/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.3167 - acc: 0.8694 - val_loss: 0.7043 - val_acc: 0.7000\n",
      "Epoch 462/500\n",
      "513/513 [==============================] - 0s 77us/step - loss: 0.3209 - acc: 0.8889 - val_loss: 0.7677 - val_acc: 0.7500\n",
      "Epoch 463/500\n",
      "513/513 [==============================] - 0s 81us/step - loss: 0.3286 - acc: 0.8791 - val_loss: 0.7307 - val_acc: 0.7000\n",
      "Epoch 464/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.3117 - acc: 0.8811 - val_loss: 0.7177 - val_acc: 0.7000\n",
      "Epoch 465/500\n",
      "513/513 [==============================] - 0s 91us/step - loss: 0.3136 - acc: 0.8811 - val_loss: 0.7041 - val_acc: 0.7000\n",
      "Epoch 466/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.3227 - acc: 0.8772 - val_loss: 0.7361 - val_acc: 0.7000\n",
      "Epoch 467/500\n",
      "513/513 [==============================] - 0s 78us/step - loss: 0.3115 - acc: 0.8791 - val_loss: 0.7105 - val_acc: 0.7000\n",
      "Epoch 468/500\n",
      "513/513 [==============================] - 0s 86us/step - loss: 0.3125 - acc: 0.9025 - val_loss: 0.7517 - val_acc: 0.7000\n",
      "Epoch 469/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.3165 - acc: 0.8791 - val_loss: 0.7191 - val_acc: 0.7000\n",
      "Epoch 470/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3142 - acc: 0.8908 - val_loss: 0.7484 - val_acc: 0.7000\n",
      "Epoch 471/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.3132 - acc: 0.8752 - val_loss: 0.6966 - val_acc: 0.7000\n",
      "Epoch 472/500\n",
      "513/513 [==============================] - 0s 77us/step - loss: 0.3123 - acc: 0.8986 - val_loss: 0.7634 - val_acc: 0.7500\n",
      "Epoch 473/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3167 - acc: 0.8694 - val_loss: 0.7049 - val_acc: 0.7000\n",
      "Epoch 474/500\n",
      "513/513 [==============================] - 0s 79us/step - loss: 0.3292 - acc: 0.8830 - val_loss: 0.7489 - val_acc: 0.7000\n",
      "Epoch 475/500\n",
      "513/513 [==============================] - 0s 83us/step - loss: 0.3116 - acc: 0.8791 - val_loss: 0.6778 - val_acc: 0.7500\n",
      "Epoch 476/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3294 - acc: 0.8869 - val_loss: 0.7594 - val_acc: 0.7500\n",
      "Epoch 477/500\n",
      "513/513 [==============================] - 0s 62us/step - loss: 0.3249 - acc: 0.8596 - val_loss: 0.6618 - val_acc: 0.7500\n",
      "Epoch 478/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3600 - acc: 0.8635 - val_loss: 0.7465 - val_acc: 0.7000\n",
      "Epoch 479/500\n",
      "513/513 [==============================] - 0s 63us/step - loss: 0.3136 - acc: 0.8889 - val_loss: 0.7336 - val_acc: 0.7500\n",
      "Epoch 480/500\n",
      "513/513 [==============================] - 0s 81us/step - loss: 0.3423 - acc: 0.8402 - val_loss: 0.6941 - val_acc: 0.7000\n",
      "Epoch 481/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513/513 [==============================] - 0s 97us/step - loss: 0.3242 - acc: 0.8811 - val_loss: 0.7336 - val_acc: 0.7000\n",
      "Epoch 482/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.3080 - acc: 0.8947 - val_loss: 0.7429 - val_acc: 0.7000\n",
      "Epoch 483/500\n",
      "513/513 [==============================] - 0s 65us/step - loss: 0.3082 - acc: 0.8830 - val_loss: 0.7367 - val_acc: 0.7000\n",
      "Epoch 484/500\n",
      "513/513 [==============================] - 0s 73us/step - loss: 0.3096 - acc: 0.8967 - val_loss: 0.7445 - val_acc: 0.7000\n",
      "Epoch 485/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3131 - acc: 0.8908 - val_loss: 0.7274 - val_acc: 0.7000\n",
      "Epoch 486/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3070 - acc: 0.8850 - val_loss: 0.7594 - val_acc: 0.7000\n",
      "Epoch 487/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3307 - acc: 0.8558 - val_loss: 0.7309 - val_acc: 0.7000\n",
      "Epoch 488/500\n",
      "513/513 [==============================] - 0s 87us/step - loss: 0.3201 - acc: 0.8713 - val_loss: 0.7687 - val_acc: 0.7500\n",
      "Epoch 489/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3106 - acc: 0.8752 - val_loss: 0.6947 - val_acc: 0.7000\n",
      "Epoch 490/500\n",
      "513/513 [==============================] - 0s 64us/step - loss: 0.3081 - acc: 0.8850 - val_loss: 0.7499 - val_acc: 0.7000\n",
      "Epoch 491/500\n",
      "513/513 [==============================] - 0s 74us/step - loss: 0.3075 - acc: 0.8811 - val_loss: 0.7311 - val_acc: 0.7000\n",
      "Epoch 492/500\n",
      "513/513 [==============================] - 0s 71us/step - loss: 0.3069 - acc: 0.8830 - val_loss: 0.7041 - val_acc: 0.7000\n",
      "Epoch 493/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.3095 - acc: 0.8947 - val_loss: 0.7551 - val_acc: 0.7500\n",
      "Epoch 494/500\n",
      "513/513 [==============================] - 0s 69us/step - loss: 0.3066 - acc: 0.8869 - val_loss: 0.7285 - val_acc: 0.7000\n",
      "Epoch 495/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3107 - acc: 0.8791 - val_loss: 0.7049 - val_acc: 0.7000\n",
      "Epoch 496/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3074 - acc: 0.8908 - val_loss: 0.7481 - val_acc: 0.7500\n",
      "Epoch 497/500\n",
      "513/513 [==============================] - 0s 66us/step - loss: 0.3081 - acc: 0.8850 - val_loss: 0.6926 - val_acc: 0.7000\n",
      "Epoch 498/500\n",
      "513/513 [==============================] - 0s 59us/step - loss: 0.3067 - acc: 0.9006 - val_loss: 0.7625 - val_acc: 0.7500\n",
      "Epoch 499/500\n",
      "513/513 [==============================] - 0s 68us/step - loss: 0.3105 - acc: 0.8850 - val_loss: 0.6787 - val_acc: 0.7500\n",
      "Epoch 500/500\n",
      "513/513 [==============================] - 0s 72us/step - loss: 0.3095 - acc: 0.8889 - val_loss: 0.7576 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() ## Inicializamos el modelo\n",
    "model.add(Dense(50, input_dim=113, activation=\"relu\")) # Capa oculta\n",
    "model.add(Dense(3, activation = 'softmax')) # Capa de salida con 3 neuronas (tenemos 3 clases en el modelo ternario)\n",
    "    \n",
    "# compilar el modelo\n",
    "model.compile(optimizer=\"Adam\",loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train,y_train, epochs=500,verbose=1, validation_data = (X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Representemos en una gráfica el accuracy según el epoch\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora, los parámetros de la red han sido elegidos al azar. En cualquier proceso de minería de datos los parámetros a utilizar son un aspecto fundamental que hay que estudiar. En este caso, los parámetros a estudiar son:\n",
    "- El número de neuronas en la capa oculta\n",
    "- El optimizador a usar [optimizers](https://keras.io/optimizers/)\n",
    "- La función de activación [activation function](https://keras.io/activations/)\n",
    "\n",
    "Haremos uso de la función GridSearchCV que busca, dados unos parámetros de entrada, la mejor combinación para un problema en concreto, según una medida de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función que crea el modelo indicando unos parámetros por defecto\n",
    "def create_model(act = 'softsign',optimizer = 'Adam', capa_oculta = 10):   \n",
    "    model = Sequential()\n",
    "    model.add(Dense(capa_oculta, input_dim=113, activation=act))\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "    \n",
    "        # compilar el modelo\n",
    "    model.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos el modelo\n",
    "model = KerasClassifier(build_fn=create_model, epochs = 10, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   34.7s\n",
      "C:\\Programas\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 1s - loss: 1.0349 - acc: 0.5205\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.9882 - acc: 0.5322\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.9554 - acc: 0.5263\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.9174 - acc: 0.5634\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.8884 - acc: 0.5867\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.8586 - acc: 0.6101\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.8308 - acc: 0.6218\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.8108 - acc: 0.6218\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.7896 - acc: 0.6452\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.7724 - acc: 0.6959\n",
      "Mejor resultado: 0.635478 usando {'act': 'tanh', 'capa_oculta': 80, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "#Definimos los parámetros\n",
    "capa_oculta = [10,20,80]\n",
    "optimizer = ['Adam','SGD']\n",
    "act = ['linear','relu','tanh','sigmoid']\n",
    "p = {'capa_oculta': capa_oculta,\n",
    "    'optimizer':optimizer,\n",
    "    'act':act\n",
    "    }\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=p,scoring='accuracy', verbose = 2, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "#resumen de los resultados\n",
    "print(\"Mejor resultado: %f usando %s\" %(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Viendo los parámetros que han salido en la ejecución anterior, modificar lo que sea necesario para obtener el mejor resultado\n",
    "model = Sequential() ## Inicializamos el modelo\n",
    "model.add(Dense(XXXXX, input_dim=113, activation=\"XXXXX\")) # Capa oculta\n",
    "model.add(Dense(3, activation = 'softmax')) # Capa de salida con 3 neuronas (tenemos 3 clases en el modelo ternario)\n",
    "    \n",
    "# compilar el modelo\n",
    "model.compile(optimizer=\"XXXX\",loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train,y_train, epochs=10,verbose=1, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo binario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red a entrenar es exactamente igual que la anterior, exceptuando que la capa de salida en vez de tener 3 neuronas (3 clases) ahora tendrá 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División del conjunto de datos en train y test\n",
    "marcadores_normalizados_L_nonL = normalizados_L_nonL.drop(columns='Class')\n",
    "y_L_nonL = normalizados_L_nonL.Class\n",
    "X_train, X_test, y_train, y_test = train_test_split(marcadores_normalizados_L_nonL, y_L_nonL, test_size=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Sabiendo que la única diferencia con respecto al modelo anterior es el número de neuronas en la capa final, haga los cambios necesarios\n",
    "\n",
    "model = Sequential() ## Inicializamos el modelo\n",
    "model.add(Dense(XXXXX, input_dim=113, activation=\"XXXXX\")) # Capa oculta\n",
    "model.add(Dense(XXXXX, activation = 'softmax')) # Capa de salida con 3 neuronas (tenemos 3 clases en el modelo ternario)\n",
    "    \n",
    "# compilar el modelo\n",
    "model.compile(optimizer=\"XXXX\",loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train,y_train, epochs=10,verbose=1, validation_data = (X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
