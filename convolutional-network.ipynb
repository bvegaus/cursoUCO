{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../admin/custom.css'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c400ed5c568c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Set CSS styling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../admin/custom.css'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mstyle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\"<style>\\n{}\\n</style>\"\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../admin/custom.css'"
     ]
    }
   ],
   "source": [
    "\"\"\"This area sets up the Jupyter environment.\n",
    "Please do not modify anything in this cell.\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Add project to PYTHONPATH for future use\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "# Import miscellaneous modules\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Set CSS styling\n",
    "with open('../admin/custom.css', 'r') as f:\n",
    "    style = \"\"\"<style>\\n{}\\n</style>\"\"\".format(f.read())\n",
    "    display(HTML(style))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "In this notebook we will become familiar with a type of *layer* for artificial neural networks called convolutional layers. The data we will attempt to model using these types of networks will be images.\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "## Images\n",
    "\n",
    "For a computer an image is a matrix of data, where each pixel is represented by one or more values:\n",
    "\n",
    "\n",
    "### Matrix with one value per pixel  =  greyscale images\n",
    "\n",
    "<img src=\"./resources/lincoln_pixel_values.png\" alt=\"Grayscale Image\" width=\"700\">[image_source](http://openframeworks.cc/ofBook/chapters/image_processing_computer_vision.html)\n",
    "\n",
    "### Matrix with three values per pixel  =  color images\n",
    " <img src=\"./resources/color_images.png\" alt=\"Decomposition of a color image\" width=\"400\">\n",
    "[image_source](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MNIST Dataset\n",
    "\n",
    "We had a brief look at this dataset in the previous notebook, and here we will go through it again with much more detail. As before, the MNIST database (Modified National Institute of Standards and Technology database) is a multiclass classification problem where we are tasked with classifying a digit ($0-9$) based on a $28\\times 28$ greyscale image:\n",
    "\n",
    ">The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    ">It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\n",
    "\n",
    "[source](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "In the following example we will load data from MNIST.\n",
    "\n",
    "* **input** $\\rightarrow$ 70000 samples of vectors\n",
    "    * Each vector has 784 dimensions\n",
    "    * Here presented as $28\\times 28$ matrices $\\rightarrow$ Greyscale images\n",
    "* **target** $\\rightarrow$ 70000 integers indicating a digit from 0 to 9\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<strong>In the following snippet of code we will:</strong>\n",
    "<ul>\n",
    "  <li>Load the MNIST dataset</li>\n",
    "  <li>Plot the 5th sample of the training set</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plots will be displaying plots within the notebook\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# NumPy is a package for manipulating N-dimensional array objects \n",
    "import numpy as np\n",
    "\n",
    "# Pandas is a data analysis package\n",
    "import pandas as pd\n",
    "\n",
    "#Library To test/verify some tasks\n",
    "import problem_unittests as tests # Used to test ouw anwsers\n",
    "\n",
    "# Mnist wrapper\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "# Code to load the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Print data shape\n",
    "print('Shape of x_train {}'.format(x_train.shape))\n",
    "print('Shape of y_train {}'.format(y_train.shape))\n",
    "print('Shape of x_test {}'.format(x_train.shape))\n",
    "print('Shape of y_test {}'.format(y_train.shape))\n",
    "\n",
    "\n",
    "# Code to plot the 5th training sample.\n",
    "fig,ax1 = plt.subplots(1,1, figsize=(7, 7))\n",
    "\n",
    "ax1.imshow(x_train[5], cmap='gray')\n",
    "title = 'Target = {}'.format(y_train[5])\n",
    "ax1.set_title(title)\n",
    "ax1.grid(which='Major')\n",
    "ax1.xaxis.set_major_locator(MaxNLocator(28))\n",
    "ax1.yaxis.set_major_locator(MaxNLocator(28))\n",
    "fig.canvas.draw()\n",
    "time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "Before we start classifying digits we need to pre-process the data.\n",
    "\n",
    "Your first task is to create a function that normalises 8-bit images from [0,255] to [0,1]:\n",
    "\n",
    "\n",
    "### Task I:  Implement an Image Normalisation Function\n",
    "<div class=\"alert alert-success\">\n",
    "**Task**: Implement a function that normalises the images to the interval [0,1].\n",
    "<ul>\n",
    "  <li>Inputs are integers in the interval [0,255]</li>\n",
    "  <li>Outputs should be floats in the interval [0,1]</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise_images(images):\n",
    "    \"\"\"Normalise input images.\n",
    "    \"\"\"\n",
    "    # Normalise image here\n",
    "\n",
    "    return images\n",
    "\n",
    "### Do *not* modify the following lines ###\n",
    "tests.test_normalize_images(normalise_images)\n",
    "\n",
    "# Normalize the data for future use\n",
    "x_train = normalise_images(x_train)\n",
    "x_test = normalise_images(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task II:  Expand the Dimension of the Input\n",
    "\n",
    "When we loaded the MNIST dataset each digit was represented by a matrix of size $(28, 28)$. However, the artificial neural network we will be building uses the concept of colour channels and feature maps even for greyscale images. This means that we have to transform $(28, 28)$ to $(28, 28, 1)$.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "**Task**: Write a piece of code that add a new dimestion to `x_train` and `x_test`.\n",
    "<ul>\n",
    "  <li>The shape of `x_train` should be $(60000, 28, 28, 1)$</li>\n",
    "  <li>The shape of `x_test` should be $(10000, 28, 28, 1)$</li>\n",
    "</ul>\n",
    "Take a look at [numpy.expand_dims()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.expand_dims.html) for how you might do this.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "x_train = None\n",
    "x_test = None\n",
    "\n",
    "\n",
    "### Do *not* modify the following lines ###\n",
    "print('Shape of x_train {}'.format(x_train.shape))\n",
    "print('Shape of y_train {}'.format(y_train.shape))\n",
    "print('Shape of x_test {}'.format(x_test.shape))\n",
    "print('Shape of y_test {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Pre-Processing\n",
    "\n",
    "To classify our digits we need to use one-hot encoding to represent the target outputs. One-hot enconding is a robust yet simple solution to represent multi-categorical targets. \n",
    "\n",
    "This enconding is an ideal representation to train a model using gradient descent algorithm with the [softmax function](http://www.cs.toronto.edu/~guerzhoy/321/lec/W04/onehot.pdf) we discussed in a previous notebook.\n",
    "\n",
    "### Example of one-hot encoding\n",
    "\n",
    "Here's an example of how a one-hot encoding scheme looks like:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{y} =\n",
    "\\left[ \\begin{array}{c} 2 \\\\ 8 \\\\ 0 \\\\ 6 \\\\ \\vdots\\end{array} \\right]\n",
    "\\Longrightarrow\n",
    "\\begin{bmatrix}\n",
    "  0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\\n",
    "  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "  0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "On the left-hand side we have a vector of target labels $\\mathbf{y}$ with $K=10$ number of classes. On the right-hand side we can see the one-hot encoded version of the $\\mathbf{y}$ vector where each element $\\in \\mathbf{y}$ has been transformed to a $K$-dimensional row vector. Only element $\\mathbf{y}_i$ has been set to 1, the rest are 0. For example, the first element of $\\mathbf{y}$ is 2, which means that the one-hot encoded vector will be all zeros except for position 2 (0-indexing). Similarly, the third element of $\\mathbf{y}$ is 0, which means the one-hot encoded vector will be all zeros except for position 0.\n",
    "\n",
    "The core idea is that you transform multi-categorical data to a combination of several single class(es). By doing this we can, for each example, see whether it belongs to any class, where 1 indicates that it does and 0 otherwise.\n",
    "\n",
    "\n",
    "### Task III:  Implement a Function for One-Hot Encoding\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "**Task**: Implement a function that one-hot encodes a vector of numbers to a matrix of $K$ classes:\n",
    "<ul>\n",
    "  <li>The first argument is vector with $N$ samples (dimensions)</li>\n",
    "  <li>The second argument is a number $K$ signifying the number of classes</li>\n",
    "  <li>For each sample of the vector you will create an array with $K$ dimensions</li>\n",
    "  <li>The one-hot encoded matrix should have zeros on all positions expect on the position indicated by the current sample in the input vector</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "* Try to implement this function by yourself. If you have doubts ask for help\n",
    "* If you are running out of time use `keras.utils.to_categorical(vector, number_classes)` like we did in the previous notebook and come back to this task later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(vector, number_classes):\n",
    "    \"\"\"Return a one-hot encoded matrix given the argument vector.\n",
    "    \"\"\"\n",
    "    # Where we will store our one-hots\n",
    "    one_hot = []\n",
    "\n",
    "    # One-hot encode `vector` here\n",
    "\n",
    "\n",
    "    # Transform list to numpy array and return it\n",
    "    return np.array(one_hot)\n",
    "\n",
    "\n",
    "### Do *not* modify the following line ###\n",
    "tests.test_one_hot(one_hot)\n",
    "\n",
    "# One-hot encode the MNIST target values\n",
    "y_train = one_hot (y_train, 10)\n",
    "y_test = one_hot(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both added an extra dimension to the input data as well as one-hot encoded the target values, let's take a look at the shapes of the data matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Shape of x_train {}'.format(x_train.shape))\n",
    "print('Shape of y_train {}'.format(y_train.shape))\n",
    "print('Shape of x_test {}'.format(x_train.shape))\n",
    "print('Shape of y_test {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an Artificial Neural Network with Convolutions and Max-Pooling\n",
    "\n",
    "\n",
    "## Convolutions\n",
    "\n",
    "If you have the task of recognising cats in an image, you might want to recognise / classify the animal regardless of its position. To do that we rely on a statistical fact: natural images are stationary [source](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution).\n",
    "\n",
    "So, if we calculate a statistic for some location in the input image, then that statistic might also be valuable to calculate at some other location. One can exploit this property to define small networks that learn features that can be applied on different parts of an image.\n",
    "\n",
    "Convolutional neural networks employs these aspects to create very efficient neural networks.\n",
    "\n",
    "In case you want to watch a short video (has captions) walking through the concepts behind convolutional networks, take a look at the following YouTube link:\n",
    "\n",
    "* [Udacity - Convolutional Networks](https://www.youtube.com/watch?v=jajksuQW4mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding windows\n",
    "\n",
    "A sliding window defines a small region of interest in an image.\n",
    "\n",
    "The region of interested is used to scan the whole image as shown in the following animation:\n",
    "\n",
    "<img src=\"./resources/sliding_window_example.gif\" alt=\"Sliding window on a greyscale image\" width=\"200\">\n",
    "\n",
    "\n",
    "If we use the sliding window to define what is the input seen by a small neural network, we have a so called convolution.\n",
    "\n",
    "\n",
    "Assuming we have a color image, and a small neural network with $k$ outputs: for every possible position of the sliding window we will have $k$ outputs.\n",
    "\n",
    "\n",
    "<img src=\"./resources/conv.png\" alt=\"Output of a convolution at for a given sliding window placement\" width=\"300\">\n",
    "\n",
    "\n",
    "After the sliding window has scanned the whole image you have 3 dimensional matrix that can be investigated further.\n",
    "\n",
    "\n",
    "<img src=\"./resources/conv2.png\" alt=\"Output of a convolution at for a given sliding window placement\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Convolution Example\n",
    "\n",
    "\n",
    "Let's assume that we have an image of $5 \\times 5=25$ pixels:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "  \\hline\n",
    "  1 & 1 & 1 & 0 & 0 \\\\ \n",
    "  \\hline\n",
    "  0 & 1 & 1 & 1 & 0\\\\\n",
    "  \\hline\n",
    "  0 & 0 & 1 & 1 & 1\\\\\n",
    "  \\hline\n",
    "  0 & 0 & 1 & 1 & 0\\\\\n",
    "  \\hline\n",
    "  0 & 1 & 1 & 0 & 0\\\\\n",
    "  \\hline\n",
    "\\end{array}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Assume that we define a small neural network that has  $3 \\times 3$ weights and a single output.\n",
    "The weight matrix is\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{array}{|c|c|c|}\n",
    "  \\hline\n",
    "  1 & 0 & 1 \\\\ \n",
    "  \\hline\n",
    "  0 & 1 & 0\\\\\n",
    "  \\hline\n",
    "  1 & 0 & 1\\\\\n",
    "  \\hline\n",
    "\\end{array}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "By feed-forwarding the network using a $3 \\times 3$  sliding window we get the following  convolved features (also know as feature map):\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{array}{|c|c|c|}\n",
    "  \\hline\n",
    "  4 & 3 & 4 \\\\ \n",
    "  \\hline\n",
    "  2 & 4 & 3\\\\\n",
    "  \\hline\n",
    "  2 & 3 & 4\\\\\n",
    "  \\hline\n",
    "\\end{array}\n",
    "\\end{equation*}\n",
    "$$\n",
    " \n",
    "\n",
    "<img src=\"./resources/Convolution_schematic.gif\" alt=\"Sliding window\" width=\"500\">\n",
    "[source](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution)\n",
    "\n",
    "The number of so-called *feature maps* produced will depend on the number of outputs of the neural network. In this case we have just one feature map.\n",
    "\n",
    "\n",
    "#### Stride and Padding\n",
    "\n",
    "We can use padding and strides to control the size of feature maps. Below are four animations that showcase the convolution operation on an input matrix using different paddings and strides:\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td><img src=\"./resources/no_padding_no_strides.gif\"></td>\n",
    "    <td><img src=\"./resources/no_padding_strides.gif\"></td>\n",
    "    <td><img src=\"./resources/same_padding_no_strides.gif\"></td>\n",
    "    <td><img src=\"./resources/padding_strides.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$padding = 0\\qquad stride = 1$</td>\n",
    "    <td>$padding = 0\\qquad stride = 2$</td>\n",
    "    <td>$padding = 1\\qquad stride = 1$</td>\n",
    "    <td>$padding = 1\\qquad stride = 2$</td>\n",
    "  </tr>\n",
    "</table>\n",
    "[source](https://github.com/vdumoulin/conv_arithmetic), [paper](https://arxiv.org/abs/1603.07285)\n",
    "\n",
    "For images, *padding* translates to how many new pixels we introduce around the edge of an image, while *stride* is how far the convolution kernel is shifted after each application.\n",
    "\n",
    "Minor terminology seen in certain kinds of literature:\n",
    "\n",
    "* **Valid** padding is equivalent to zero padding\n",
    "* **Same** padding means that the padding used is a function of the kernel size so that the output size has the same size as the input\n",
    "\n",
    "\n",
    "### Computing the Size of the Convolutions\n",
    "\n",
    "To compute the size of the feature map resulting from a convolution we need to know the input size, the size of the kernel (filter), the stride, and the padding:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "output = \\frac{1}{stride} (input -  kernel + 2 * padding) + 1\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "For 2-d inputs the height and width can be calculated like this:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "height_{new} =  \\frac{1}{stride} (height_{input} - height_{kernel} + 2 * padding)+1\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br class=\"math\" />\n",
    "$$\n",
    "\\begin{equation*}\n",
    "widht_{new} =  \\frac{1}{stride} (widht_{input} - widht_{kernel} + 2 * padding)+1\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Example :\n",
    "\n",
    "Let us assume that we have an image that is $6 \\times 6$. If we pad this image with a single pixel and then convolve it with a $3 \\times 3$ kernel using a stride of $2$, we get the following $3 \\times 3$ feature map:\n",
    "\n",
    "<img src=\"./resources/odd.gif\" alt=\"Examples math\" width=\"300\">\n",
    "\n",
    "We can compute the output size using the equations above:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "height_{new} &= \\frac{1}{stride} (height_{input} - height_{kernel} + 2 * padding) + 1 \\\\\n",
    "&= \\frac{1}{2} (6 - 3 + 2 * 1) + 1 \\\\\n",
    "&= \\frac{1}{2}(5)+1 \\\\\n",
    "&= 3\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Notice that we round floating numbers, so we go from 3.5 to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "It has become common practice to use pooling layer between convolutional layers.\n",
    "\n",
    "Sucessful convolutional neural networks like [alexnet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks), [VGG16, VGG19](https://arxiv.org/abs/1409.1556) employed this technique.\n",
    "\n",
    "Pooling layers also depend on sliding windows, but instead of using the window as inputs for neurons, the sliding input data goes through a `max`, `mean`, or some other operator.\n",
    "\n",
    "### A Max-Pooling Example\n",
    "\n",
    "<img src=\"./resources/pooling.gif\" alt=\"Max Pooling\" width=\"300\">\n",
    "\n",
    "Max-pooling has several advantages:\n",
    "* If you have images where the same class has similar images with small shifts in the pixel position, max pooling will mitigate small translations\n",
    "* It introduces zero parameters to the model since the max and mean operators are fixed functions that do not depend on weights\n",
    "* It reduces the amount of data that need to be processed in the next layer, while assuring some pixel translation invariance.\n",
    "* They are normally used with zero padding (aka valid padding).\n",
    "* They follow the same dimensional maths as convolutions\n",
    "\n",
    "[You can read more about pooling operators here](http://cs231n.github.io/convolutional-networks/#pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Convolutional Network\n",
    "\n",
    "We will implement the following convolution network:\n",
    "\n",
    "<img src=\"./resources/mnist_net.png\" alt=\"CNN\" width=\"1280\">\n",
    "\n",
    "The components of this network can be seen below:\n",
    "\n",
    "Define an input:\n",
    "\n",
    "* `input_x = Input(shape=sample_shape)`\n",
    "* `sample_shape` is an input parameter\n",
    "    \n",
    "Generate 32 kernel maps using a convolutional layer:\n",
    "\n",
    "* The convolution uses a $3 \\times 3$ kernel, stride 1, valid padding, and [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) activation\n",
    "* Use [Conv2D](https://keras.io/layers/convolutional/) from Keras\n",
    "* `output_layer = Conv2D(PARAMETERS)(input_layer)`\n",
    "    \n",
    "Generate 64 kernel maps using a convolutional layer:\n",
    "\n",
    "* The convolution uses a $3 \\times 3$ kernel, stride 1, valid padding, and [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) activation\n",
    "* Use [Conv2D](https://keras.io/layers/convolutional/)\n",
    "* `output_layer = Conv2D(PARAMETERS)(input_layer)`\n",
    "\n",
    "Reduce the feature maps using max-pooling:\n",
    "\n",
    "* The max-pooling should us a $2 \\times 2$ kernel, stride `None`, and valid padding\n",
    "* Use [MaxPooling2D](https://keras.io/layers/pooling/#maxpooling2d)\n",
    "* `output_layer = MaxPooling2D(PARAMETERS)(input_layer)`\n",
    "\n",
    "Flatten the feature map:\n",
    "\n",
    "* [Flatten](https://keras.io/layers/core/#flatten)\n",
    "\n",
    "Fully-connected, i.e. `Dense`, to 128 dimensions:\n",
    "\n",
    "* [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) activation\n",
    "* [Dense](https://keras.io/layers/core/#dense)\n",
    "\n",
    "Fully-connected, i.e. `Dense`, to $K$ classes (argument) dimensions:\n",
    "\n",
    "* [Softmax](https://en.wikipedia.org/wiki/Softmax_function) activation\n",
    "* [Dense](https://keras.io/layers/core/#dense)\n",
    "\n",
    "\n",
    "### Task IV:  Implement a Convolutional Neural Network Model\n",
    "\n",
    "It is time to implement our first convolutional neural network.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<strong>Task:</strong> Create a function `net_1()` that implements the network specified above.\n",
    "Make sure to refer back to earlier notebooks if you are unsure about what to do.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Keras library\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "\n",
    "\n",
    "def net_1(sample_shape, nb_classes):\n",
    "    # Define the network input to have `sample_shape´ shape\n",
    "    input_x = None\n",
    "    \n",
    "    # Create network internals here\n",
    "    x = None\n",
    "    \n",
    "    # Dense `nb_classes`\n",
    "    probabilities = Dense(nb_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Define the output\n",
    "    model = Model(inputs=input_x, outputs=probabilities)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>In the following code snippet we will:</strong>\n",
    "<ul>\n",
    "  <li>Create the network using the function you just made</li>\n",
    "  <li>Display a summary of the network</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shape of sample\n",
    "sample_shape = x_train[0].shape \n",
    "\n",
    "# Construct net\n",
    "model = net_1(sample_shape, 10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task V: Define Hyperparameters and Train the Network\n",
    "\n",
    "We need to define hyperparameters so our network can learn.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<strong>Task:</strong> Tune in the hyper-parameters until your `loss` and `val_loss` are both converging to low numbers:\n",
    "<ul>\n",
    "  <li>Batch size</li>\n",
    "  <li>Number of training epochs</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Keep in mind that training these kinds of networks will take longer than the ones we have looked at so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = None\n",
    "epochs = None\n",
    "\n",
    "### Do *not* modify the following lines ###\n",
    "\n",
    "# There is no learning rate because we are using the recommended\n",
    "# values for the Adadelta optimiser more information here:\n",
    "# https://keras.io/optimizers/\n",
    "\n",
    "# We need to compile our model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "logs = model.fit(x_train, y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 verbose=2,\n",
    "                 validation_split=0.1)\n",
    "\n",
    "# Plot our losses and accuracy\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "pd.DataFrame(logs.history).plot(ax=ax)\n",
    "ax.grid(linestyle='dotted')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Assess performance\n",
    "print('='*80)    \n",
    "print('Assesing Test dataset...')\n",
    "print('='*80)    \n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should We Use Max-Pooling?\n",
    "\n",
    "There is a recent discussion if max-pooling is a good solution for reducing the amount of data between layers of a network. Some recent approaches show that a similar, and sometimes better performance, can be achieved by using convolutions with strides larger than 1.\n",
    "\n",
    "\n",
    "> Getting rid of pooling. Many people dislike the pooling operation and think that we can get away without it. For example, [Striving for Simplicity: The All Convolutional Net](http://arxiv.org/abs/1412.6806) proposes to discard the pooling layer in favor of architecture that only consists of repeated CONV layers. To reduce the size of the representation they suggest using larger stride in CONV layer once in a while. Discarding pooling layers has also been found to be important in training good generative models, such as variational autoencoders (VAEs) or generative adversarial networks (GANs). It seems likely that future architectures will feature very few to no pooling layers.\n",
    "\n",
    "[source](http://cs231n.github.io/convolutional-networks/#pool)\n",
    "\n",
    "\n",
    "### Task VI: Implement a Convolutional Network Without Max-Pooling\n",
    "\n",
    "Implement a convolutional neural network without pooling layers:\n",
    "\n",
    "<img src=\"./resources/mnist_net2.png\" alt=\"CNN\" width=\"1280\">\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<strong>Task:</strong> Replicate the network we made before (`net_1()`), but this time:\n",
    "<ul>\n",
    "  <li>Remove max pooling and add stride(s) of 2 to the second convolution block (see [Conv2D](https://keras.io/layers/convolutional/))</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net_2(sample_shape, nb_classes):\n",
    "    # Define the network input to have `sample_shape` shape\n",
    "    input_x = None\n",
    "    \n",
    "    # Create network internals here\n",
    "    x = None\n",
    "\n",
    "    # Dense number_classes\n",
    "    probabilities = Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "    # Define the output\n",
    "    model = Model(inputs=input_x, outputs=probabilities)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>In the following code snippet we will:</strong>\n",
    "<ul>\n",
    "  <li>Create the network using the function you just made</li>\n",
    "  <li>Display a summary of the network</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shape of sample\n",
    "sample_shape = x_train[0].shape \n",
    "\n",
    "# Construct net\n",
    "model = net_2(sample_shape, 10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task VII: Define Hyperparameters and Train the New Network\n",
    "\n",
    "As before, we need to define some hyperparameters and train the network. Feel free to reuse the hyperparameters you found before.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<strong>Task:</strong> Tune in the hyper-parameters until your `loss` and `val_loss` are both converging to low numbers:\n",
    "<ul>\n",
    "  <li>Batch size</li>\n",
    "  <li>Number of training epochs</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = None\n",
    "epochs = None\n",
    "\n",
    "### Do *not* modify the following lines ###\n",
    "\n",
    "# As always we need to compile our model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "logs = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_split = 0.1,)\n",
    "\n",
    "# Plot our losses and accuracy\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "pd.DataFrame(logs.history).plot(ax=ax)\n",
    "ax.grid(linestyle='dotted')\n",
    "ax.legend()\n",
    "fig.canvas.draw()\n",
    "\n",
    "\n",
    "# Assess performance\n",
    "print('='*80)    \n",
    "print('Assesing Test dataset...')\n",
    "print('='*80)    \n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CIFAR\n",
    "\n",
    "The following explanantion of Cifar 10 comes from [official cifar page](https://www.cs.toronto.edu/~kriz/cifar.html):\n",
    "\n",
    "The CIFAR-10 and CIFAR-100 are labeled subsets of the <a href=\"http://people.csail.mit.edu/torralba/tinyimages/\">80 million tiny images</a> dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
    "\n",
    "\n",
    "## The CIFAR10 Dataset\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 $32 \\times 32$ colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.</li>\n",
    "\n",
    "Here are the classes in the dataset, as well as 10 random images from each:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td class=\"cifar-class-name\">airplane</td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane1.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane2.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane3.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane4.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane5.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane6.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane7.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane8.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane9.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane10.png\" class=\"cifar-sample\" /></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td class=\"cifar-class-name\">automobile</td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile1.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile2.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile3.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile4.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile5.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile6.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile7.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile8.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile9.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile10.png\" class=\"cifar-sample\" /></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td class=\"cifar-class-name\">bird</td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird1.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird2.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird3.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird4.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird5.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird6.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird7.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird8.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird9.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird10.png\" class=\"cifar-sample\" /></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td class=\"cifar-class-name\">cat</td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat1.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat2.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat3.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat4.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat5.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat6.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat7.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat8.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat9.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat10.png\" class=\"cifar-sample\" /></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td class=\"cifar-class-name\">deer</td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer1.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer2.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer3.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer4.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer5.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer6.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer7.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer8.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer9.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer10.png\" class=\"cifar-sample\" /></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td class=\"cifar-class-name\">dog</td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog1.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog2.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog3.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog4.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog5.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog6.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog7.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog8.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog9.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog10.png\" class=\"cifar-sample\" /></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td class=\"cifar-class-name\">frog</td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog1.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog2.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog3.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog4.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog5.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog6.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog7.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog8.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog9.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog10.png\" class=\"cifar-sample\" /></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td class=\"cifar-class-name\">horse</td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse1.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse2.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse3.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse4.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse5.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse6.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse7.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse8.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse9.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse10.png\" class=\"cifar-sample\" /></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td class=\"cifar-class-name\">ship</td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship1.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship2.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship3.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship4.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship5.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship6.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship7.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship8.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship9.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship10.png\" class=\"cifar-sample\" /></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td class=\"cifar-class-name\">truck</td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck1.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck2.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck3.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck4.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck5.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck6.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck7.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck8.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck9.png\" class=\"cifar-sample\" /></td>\n",
    "        <td><img src=\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck10.png\" class=\"cifar-sample\" /></td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br/>\n",
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example we will load data from CIFAR10.\n",
    "\n",
    "* **input** $\\rightarrow$ 60000 samples of 3072 dimensional vectors.\n",
    "    * Here presented as $32 \\times 32 \\times 3$ matrices $\\rightarrow$ Colour images\n",
    "* **target** $\\rightarrow$ 60000 scalars indicating a class from 0 to 9\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>In the following code we will:</strong>\n",
    "<ul>\n",
    "  <li>Load the CIFAR10 dataset</li>\n",
    "  <li>Plot the 5th sample of training set</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "target_2_class = {0:'airplane',\n",
    "                  1:'automobile',\n",
    "                  2:'bird',\n",
    "                  3:'cat',\n",
    "                  4:'deer',\n",
    "                  5:'dog',\n",
    "                  6:'frog',\n",
    "                  7:'horse',\n",
    "                  8:'ship',\n",
    "                  9:'truck'}\n",
    "\n",
    "# Code to plot the 5th training sample.\n",
    "fig,ax1 = plt.subplots(1,1, figsize=(7,7))\n",
    "ax1.imshow(x_train[5])\n",
    "target = y_train[5][0]\n",
    "title = 'Target is {} - Class {}'.format(target_2_class[target],target )\n",
    "ax1.set_title(title)\n",
    "ax1.grid(which='Major')\n",
    "ax1.xaxis.set_major_locator(MaxNLocator(32))\n",
    "ax1.yaxis.set_major_locator(MaxNLocator(32))\n",
    "fig.canvas.draw()\n",
    "time.sleep(0.1)\n",
    "\n",
    "print('Shape of x_train {}'.format(x_train.shape))\n",
    "print('Shape of y_train {}'.format(y_train.shape))\n",
    "print('Shape of x_test {}'.format(x_train.shape))\n",
    "print('Shape of y_test {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task VIII: One-Hot Encode the Target Values\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<strong>Task:</strong> Use the `one_hot()` function you created earlier to encode:\n",
    "<ul>\n",
    "  <li>`y_test`</li>\n",
    "  <li>`y_train`</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = None\n",
    "y_test = None\n",
    "\n",
    "### Do *not* modify the following line ###\n",
    "# Print data sizes\n",
    "print('Shape of x_train {}'.format(x_train.shape))\n",
    "print('Shape of y_train {}'.format(y_train.shape))\n",
    "print('Shape of x_test {}'.format(x_train.shape))\n",
    "print('Shape of y_test {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task IX: Normalise the Images\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<strong>Task:</strong> Use the `normalise_images()` function you created earlier to normalise the images in:\n",
    "<ul>\n",
    "  <li>`x_test`</li>\n",
    "  <li>`x_train`</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = None\n",
    "x_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task X: Create Your Convolutional Neural Network for CIFAR10\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "  <strong>Task:</strong> Create a neural network model to train on CIFAR using what we have learned so far.\n",
    "<ul>\n",
    "  <li>Create a new network using either `net_1()` or `net_2()`</li>\n",
    "  <li>Display a summary of the network</li>\n",
    "  <li>Compile the model using eiter `Adadelta`, `Adagrad`, or `Adam` as your optimiser</li>\n",
    "</ul>\n",
    "Some of the code is filled in already so alter what you need.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shape of samples\n",
    "sample_shape = x_train[0].shape \n",
    "\n",
    "# Construct net\n",
    "model = None\n",
    "model.summary()\n",
    "\n",
    "# We need to compile our model network:\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task XI: Train Your Model\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "  <strong>Task:</strong> : Train the model created in the previous cell on CIFAR10.\n",
    "  <ul>\n",
    "  <li>Train the network using `epochs = 30`</li>\n",
    "  <li>Train the network using a `batch_size = 128`</li>\n",
    "  <li>Use `validation_split = 0.2` when calling `fit()`</li>\n",
    "  <li>Plot the losses and accuracy</li>\n",
    "  <li>Assess the performance on the test set</li>\n",
    "</ul>\n",
    "We recommend that you do not copy-paste from the cells above, but rather re-write the code yourself. You can always take a look at earlier cells if you are unsure about what to do.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the code within this cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics to Think About\n",
    "\n",
    "* Which model performs better?\n",
    "* Which optimiser performs better?\n",
    "* Is there any evidence of overfitting?\n",
    "* How can we improve the performance even further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If You Have Time\n",
    "\n",
    "\n",
    "* Create a new network net_3():\n",
    "    * Add more convolutional layers\n",
    "    * Try adding [dropout](https://keras.io/layers/core/#dropout) after `Flatten` layer\n",
    "        * You can this as reference : https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\n",
    "    * Try adding [batch normalization](https://keras.io/layers/normalization/).\n",
    "        * https://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras\n",
    "        * https://www.quora.com/How-do-I-apply-Batch-Normalization-to-the-convolutional-layer-of-a-CNN\n",
    "* Take a look at this [article](http://sebastianruder.com/optimizing-gradient-descent/) to learn more about optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
